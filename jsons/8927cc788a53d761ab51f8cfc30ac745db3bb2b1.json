{
    "paper_id": "8927cc788a53d761ab51f8cfc30ac745db3bb2b1",
    "metadata": {
        "title": "How well can we forecast the COVID-19 pandemic with curve fitting and recurrent neural networks?",
        "authors": [
            {
                "first": "Zhuowen",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Michigan State University",
                    "location": {
                        "postCode": "48824",
                        "settlement": "East Lansing",
                        "region": "MI",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Kieran",
                "middle": [],
                "last": "Nehil-Puleo",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Michigan State University",
                    "location": {
                        "postCode": "48824",
                        "settlement": "East Lansing",
                        "region": "MI",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Yangzhi",
                "middle": [],
                "last": "Zhao",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Lawrence Berkeley National Laboratory",
                    "location": {
                        "postCode": "94720",
                        "settlement": "Berkeley",
                        "region": "CA",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Predictions of the COVID-19 pandemic in USA are compared using curve fitting and various recurrent neural networks (RNNs) including the standard long short-term memory (LSTM) RNN and 10 types of slim LSTM RNNs. The curve fitting method predicts the pandemic would end in early summer but the exact date and scale vary with the evolving data used for fitting. All LSTM RNNs result in short-term (8 to 10 days) predictions with comparable accuracies (smaller than 10 %) to curve fitting-they do not show advantage over curve fitting.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Since the first report in late January, COVID-19 has spread to the whole world and become a global pandemic. The accurate prediction of the scale and time of the pandemic is helpful information to the the public to cope with the crisis. In this paper, we compare the predictions with two methods, i.e., curve fitting of a modified exponential function and recurrent neural network (RNN). The curve fitting method is basically an extrapolation based on a well-fitted analytical function that can be used for full-range predictions (including short-term predictions up to any time before end). If no noise added, the function or its derivative (delta values if not differentiable) is not expected to capture local fluctuations. RNN is typically used for predicting serial events, which is potentially capable for capturing daily cases fluctuations of the COVID-19 pandemic. This paper aims at (i) evaluating full-range predictions with curve fitting as well as at (ii) comparing short-term (8-10 days) forecasting with smooth curve fitting (no noise added) and RNN. In order to overcome the possible long-term learning difficulty due to gradient vanishing or gradient explosion of simple RNN, long short-term memory (LSTM) technique is usually adopted [1] . In addition to the standard LSTM RNN, we also experiment with simplified (slim LSTM) networks-less parameters in the gate (8 types) and memory cell (2 types) structures [2, 3] .",
            "cite_spans": [
                {
                    "start": 1059,
                    "end": 1063,
                    "text": "RNN.",
                    "ref_id": null
                },
                {
                    "start": 1250,
                    "end": 1253,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 1425,
                    "end": 1428,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 1429,
                    "end": 1431,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Data source is located at Github repository https:// github.com/CSSEGISandData/COVID-19, which is originally from Johns Hopkins University Coronavirus Resource Center. It has confirmed patients and deaths data in time series in the category of regions and the corresponding countries. We plot confirmed patients from Hubei province of China (first report location), China, USA, and Italy with respect to the days since first case reported in each region shown in Fig. 1 . Italy curve is shorter than the other regions because the first patient reported for Italy was on January 31 while it was January 22 for both China and USA. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 463,
                    "end": 469,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "COVID-19 data"
        },
        {
            "text": "Langel [1] reported a model in the form of a modified exponential function Eq. (1) that describes the evolution of the cumulative cases of the pandemic.",
            "cite_spans": [
                {
                    "start": 7,
                    "end": 10,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Full-range prediction with curve fitting"
        },
        {
            "text": ",where x \u221e , x 0 , \u03c4 are final fitting parameter (ultimate number of cases), initial fitting parameter (for numerical purpose), and a parameter that controls the \"curvature\" of the curve. The idea behind this model is that the cumulative patients/death curve in principle follows the trajectory of exponential function whose derivative (daily increment) looks like a skewed normal distribution curve.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Full-range prediction with curve fitting"
        },
        {
            "text": "The model is fitted (least square) with cumulative patients data of three different lengths (82, 88, 98-day) to investigate its performance with evolving data. Table 1 shows the three optimized parameter sets and one parameter set for death data. The metric to evaluate the goodness of the fitting, i. e., normalized root mean square error (RMSE) is calculated based on true values and the fitted values within the same time frametheir RMSE divided by the mean of true values. The normalized RMSE is only 0.04 (or 4 %) by fitting with confirmed infection data and 0.02 by fitting with death data up to April 28 (98-day). The daily patients curve (red dashed line in Fig. 2 top)-delta values of the fitted function-suggests the pandemic in USA would end around June 30, 2020. The daily death curve (blue dashed line in Fig. 2 bottom) suggests the America would see zero death since July. Unfortunately, the model also predicts about 1.3 million infections and 77 thousand deaths in America at the end of COVID-19 pandemic based on the 98-day data. However, the ultimate number of patients by prediction increases from 1.0 to 1.3 million with more data (16 days), see Table 1 . This suggests the COVID-19 is a quickly evolving situation, thus one-time full-range prediction may not be very reliable using the curve fitting.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 160,
                    "end": 167,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 666,
                    "end": 672,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 818,
                    "end": 824,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 1166,
                    "end": 1173,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Full-range prediction with curve fitting"
        },
        {
            "text": "The dataset is split into training/fitting (90 %) and testing (10 %) subsets (to evaluate the short-term prediction). The 2 . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Short-term prediction with curve fitting"
        },
        {
            "text": "The copyright holder for this preprint this version posted May 18, 2020. . function is firstly fitted with the training set to get optimized parameters. Then the prediction from extrapolation are evaluated using normalized RMSE against testing set . Figure 3 shows the short-term predictions (9-day and 10-day) using 82-day and 88day data for fitting respectively. The normalized RMSE values vary with different fitting data lengths and they are both smaller than 10 %. Moreover, curve fitting tends to underestimate cases for short-term prediction.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 250,
                    "end": 258,
                    "text": "Figure 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "(which was not certified by peer review)"
        },
        {
            "text": "forget gate",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Short-term prediction with standard LSTM RNN"
        },
        {
            "text": "inner product sum The standard LSTM RNN is built on Keras with Tensor-Flow as the backend software. The network has an architecture of four LSTM cells followed by a dense output layer. Figure 4 shows the structure for one of the LSTM cells. Hyperbolic tangent (tanh) function (default in Keras library) is used as activation function for all gates and memory cells. We also tested sigmoid function, another popular choice for activation function in RNN, but the prediction was totally off with the same network architecture. \"Adam optimizer\" from Keras library is adopted for the backpropagation updates and mean squared error is used as the loss function. The formula of standard LSTM RNN are summarized in the following [4, 5] .",
            "cite_spans": [
                {
                    "start": 722,
                    "end": 725,
                    "text": "[4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 726,
                    "end": 728,
                    "text": "5]",
                    "ref_id": "BIBREF5"
                }
            ],
            "ref_spans": [
                {
                    "start": 185,
                    "end": 193,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Short-term prediction with standard LSTM RNN"
        },
        {
            "text": ", where f t , i t , o t , c t denotes forget, input, output gate, and cell structure at time t in LSTM cells respectively. h t is state variable The short-term predictions are made using dataset of two different lengths (91 and 98 days) respectively. Each dataset is normalized into the range of 0 to 1. Four new subsets X train , Y train , X test , and Y test are made from each dataset, among which X train and Y train are used to train the RNN network, and Y test is used for evaluating the predictions from Y train . Take 91day data for example, data points in the first 82 days are used as training set (90 %) and the rest (9 days) are used as testing set (10 %). From the training set, X train and Y train are made by removing the last day and the first day from the original training set respectively, such that Y train is one day ahead of X train for each data point. In other words, Y train contains true values of the one-day evolution of X train (both subsets have 81 days). X test and Y test are made in the same way from the training test (8 days). Figure 5 shows evaluation results against the training and testing sets. The normalized RMS E train and RMS E test demonstrate how well the RNN network is trained and how well the trained network forecasts respectively. It is seen from Fig. 5 , both evaluations are of the same level of magnitude compared to curve fitting.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 1062,
                    "end": 1070,
                    "text": "Figure 5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 1298,
                    "end": 1304,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Short-term prediction with standard LSTM RNN"
        },
        {
            "text": "LSTM RNN can be simplified by reducing parameters in the three gates (forget, input, and output gate) or in memory cells [2, 3] , which is expected to accelerate the convergence. There are 10 types of simplified LSTM RNN used in this study, which are denoted as LSTM1, LSTM2, LSTM3, LSTM4, LSTM4a, LSTM5, LSTM5a, LSTM6, LSTM10, LSTM11 respectively. The detailed formula for each type can be found in Section 7 Appendix. Figure 6 shows the prediction of patients in USA with slim LSTM RNN(s) using 82-day data. They all have same level of accuracies compared to standard LSTM and curve fitting, 3 . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 124,
                    "text": "[2,",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 125,
                    "end": 127,
                    "text": "3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [
                {
                    "start": 420,
                    "end": 428,
                    "text": "Figure 6",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Short-term prediction with slim LSTM RNN"
        },
        {
            "text": "The copyright holder for this preprint this version posted May 18, 2020. among which LSTM6 demonstrates the best prediction accuracy (least normalized RMSE against testing set). All slim LSTM RNN networks tend to underestimate the future cases compared to the reported data (magenta dash-dotted line is below the black solid line for all models in Fig. 6 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 348,
                    "end": 354,
                    "text": "Fig. 6",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "(which was not certified by peer review)"
        },
        {
            "text": "This paper assesses full-range predictions of COVID-19 pandemic in USA with curve fitting using data of different lengths and compares short-term (8 to 10 days) predictions with curve fitting and 11 different LSTM recurrent neural networks. The full-range predictions using the latest data (up to April 28) suggests the pandemic in USA would end around the 160th day (June 30, 2020) since the first reported case and there would be over 1.3 \u00d7 10 6 infections and 7.7 \u00d7 10 4 deaths . Nevertheless, as the pandemic is evolving quickly on the daily basis and there are many variables (policies, people's compliance to stay-home order etc.) that affect the real situation, one-time prediction of the ultimate date and scale might not be reliable enough. Therefore, it is advisable to fine tune the full-range predictions with the evolving data for the curve fitting method. In terms of shortterm predictions, LSTM6 does the prediction with the highest accuracy among the standard LSTM RNN and 10 types of slim LSTM RNNs tested in this study. However, LSTM RNNs do not show advantage over curve fitting for this type of predictions. Curve fitting might be better to fit the true distribution of how COVID-19 infections \"behave\" because it does not overfit on the training set compared to RNN.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Summary"
        },
        {
            "text": "We greatly appreciate the inspiration and help from professor Fathi S. Salem at Michigan State University. The computational calculation was supported by Google Compute Engine. Jupyter notebook for this study is hosted at https: //github.com/zhuowenzhao/COVID19-prediction",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgement"
        },
        {
            "text": "Gate equations parameter-reductions [1] : LSTM1",
            "cite_spans": [
                {
                    "start": 36,
                    "end": 39,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "LSTM4a f t = \u03b1, \u22121 < \u03b1 < 1(default = 0.96) i t = \u03c3 in (u i h t\u22121 ) c t = \u03c3(W c x t + U c h t\u22121 + b c ) c t = i t c t + f t c t\u22121 o t = 1.0 h t = o t \u03c3(c t )",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "Memory cell equations parameter reductions: LSTM10",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "5 . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        },
        {
            "text": "The copyright holder for this preprint this version posted May 18, 2020. . https://doi.org/10.1101/2020.05.14.20102541 doi: medRxiv preprint",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Extrapolation of infection data for the covid-19 virus and estimate of the pandemic time scale",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Langel",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1101/2020.03.26.20044081"
                ]
            }
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Gate-variants of gated recurrent unit (gru) neural networks",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Dey",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "M"
                    ],
                    "last": "Salem",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS",
            "volume": "",
            "issn": "",
            "pages": "1597--1600",
            "other_ids": {
                "DOI": [
                    "10.1109/MWSCAS.2017.8053243"
                ]
            }
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A basic recurrent neural network model",
            "authors": [
                {
                    "first": "F",
                    "middle": [
                        "M"
                    ],
                    "last": "Salem",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1612.09022"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Bengio",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Frasconi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "A Field Guide to Dynamical Recurrent Neural Networks",
            "authors": [],
            "year": 2001,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural Computation",
            "volume": "9",
            "issn": "",
            "pages": "1735--1780",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "Top: confirmed COVID-19 cases of the whole world and four regions since first case was reported. Bottom: cumulative patients and new cases reported per day of USA since the first case was reported. Data in both plots are up to April 28.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Short-term prediction of COVID-19 patients in USA by fitting with 82-day (top) and 88-day data (bottom).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Schematics of LSTM RNN structure.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Prediction of cumulative patients in USA with LSTM using 82-days (top) and 88-day data (bottom).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "o are weight matrices and b f , b i , b o , b c are bias vectors. \u03c3 in (\u00b7) (for gates, see Fig. 4) and \u03c3(\u00b7) are activation functions.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Prediction of cumulative patients in USA with slim LSTM RNN(s) using 82-day data.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Optimized parameters x \u221e , x 0 , \u03c4 based on cumulative patients (82, 88, 98-day) data and death (60-day) data in USA.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Figure 2: Prediction of total patients (top) and deaths (bottom) due to COVID-19 in USA using data up to April 28 (98 days for patient data and 60 days for death data). The first reported death in USA was on February 29 according to the JHU dataset (this date varies with other media).",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}