{
    "paper_id": "373ed692426031a9804a75b3baf26d26e40e99d5",
    "metadata": {
        "title": "Spectral Clustering by Subspace Randomization and Graph Fusion for High-Dimensional Data",
        "authors": [
            {
                "first": "Xiaosha",
                "middle": [],
                "last": "Cai",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "South China Agricultural University",
                    "location": {
                        "settlement": "Guangzhou",
                        "country": "China"
                    }
                },
                "email": "xiaoshacai@hotmail.com"
            },
            {
                "first": "Dong",
                "middle": [],
                "last": "Huang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "South China Agricultural University",
                    "location": {
                        "settlement": "Guangzhou",
                        "country": "China"
                    }
                },
                "email": "huangdonghere@gmail.com"
            },
            {
                "first": "Chang-Dong",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Sun Yat-sen University",
                    "location": {
                        "settlement": "Guangzhou",
                        "country": "China"
                    }
                },
                "email": "changdongwang@hotmail.com"
            },
            {
                "first": "Chee-Keong",
                "middle": [],
                "last": "Kwoh",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Nanyang Technological University",
                    "location": {
                        "settlement": "Singapore",
                        "country": "Singapore"
                    }
                },
                "email": "asckkwoh@ntu.edu.sg"
            }
        ]
    },
    "abstract": [
        {
            "text": "Subspace clustering has been gaining increasing attention in recent years due to its promising ability in dealing with high-dimensional data. However, most of the existing subspace clustering methods tend to only exploit the subspace information to construct a single affinity graph (typically for spectral clustering), which often lack the ability to go beyond a single graph to explore multiple graphs built in various subspaces in high-dimensional space. To address this, this paper presents a new spectral clustering approach based on subspace randomization and graph f usion (SC-SRGF) for high-dimensional data. In particular, a set of random subspaces are first generated by performing random sampling on the original feature space. Then, multiple K-nearest neighbor (K-NN) affinity graphs are constructed to capture the local structures in the generated subspaces. To fuse the multiple affinity graphs from multiple subspaces, an iterative similarity network fusion scheme is utilized to achieve a unified graph for the final spectral clustering. Experiments on twelve real-world high-dimensional datasets demonstrate the superiority of the proposed approach. The MATLAB source code is available at https://www.researchgate.net/publication/338864134.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "Data clustering is a fundamental yet still very challenging problem in data mining and knowledge discovery [13] . A large number of clustering techniques have been developed in the past few decades [2] [3] [4] [5] [6] [8] [9] [10] [11] [12] [14] [15] [16] [17] [18] [21] [22] [23] [24] , out of which the spectral clustering has been a very important category with its effectiveness and robustness in dealing with complex data [3, 6, 14, 18, 22] . In this paper, we focus on the spectral clustering technique, especially for high-dimensional scenarios.",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 111,
                    "text": "[13]",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 198,
                    "end": 201,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 202,
                    "end": 205,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 206,
                    "end": 209,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 210,
                    "end": 213,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 214,
                    "end": 217,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 218,
                    "end": 221,
                    "text": "[8]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 222,
                    "end": 225,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 226,
                    "end": 230,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 231,
                    "end": 235,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 236,
                    "end": 240,
                    "text": "[12]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 241,
                    "end": 245,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 246,
                    "end": 250,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 251,
                    "end": 255,
                    "text": "[16]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 256,
                    "end": 260,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 261,
                    "end": 265,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 266,
                    "end": 270,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 271,
                    "end": 275,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 276,
                    "end": 280,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 281,
                    "end": 285,
                    "text": "[24]",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 427,
                    "end": 430,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 431,
                    "end": 433,
                    "text": "6,",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 434,
                    "end": 437,
                    "text": "14,",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 438,
                    "end": 441,
                    "text": "18,",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 442,
                    "end": 445,
                    "text": "22]",
                    "ref_id": "BIBREF21"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In high-dimensional data, it is often recognized that the cluster structures of data may lie in some low-dimensional subspaces [3] . Starting from this assumption, many efforts have been made to enable the spectral clustering for highdimensional data by exploiting the subspace information from different technical perspectives [1, 3, 4, 15, 17, 21, 23] . Typically, a new affinity matrix is often learned with the subspace structure taken into consideration, upon which the spectral clustering process is then performed. For example, Liu et al. [17] proposed a lowrank representation (LRR) approach to learn an affinity matrix, whose goal is to segment the data points into their respective subspaces. Chen et al. [1] exploited K-nearest neighbor (K-NN) based sparse representation coefficient vectors to build an affinity matrix for high-dimensional data. He et al. [4] used information theoretic objective functions to combine structured LRRs, where the global structure of data is incorporated. Li et al. [15] presented a subspace clustering approach based on Cauchy loss function (CLF) to alleviate the potential noise in high-dimensional data. Elhamifar and Vidal [3] proposed the sparse subspace clustering (SSC) approach by incorporating the low-dimensional neighborhood information, where each data point is represented by a combination of other points in its own subspace and a new similarity matrix is then constructed. You et al. [23] extended the SSC approach by introducing orthogonal matching pursuit (OMP) to learn a subspace-preserving representation. Wang et al. [21] combined SSC and LRR into a novel low-rank sparse subspace clustering (LRSSC) approach.",
            "cite_spans": [
                {
                    "start": 127,
                    "end": 130,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 328,
                    "end": 331,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 332,
                    "end": 334,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 335,
                    "end": 337,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 338,
                    "end": 341,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 342,
                    "end": 345,
                    "text": "17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 346,
                    "end": 349,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 350,
                    "end": 353,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 546,
                    "end": 550,
                    "text": "[17]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 715,
                    "end": 718,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 868,
                    "end": 871,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "text": "[15]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 1170,
                    "end": 1173,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "text": "[21]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Although these methods [1, 3, 4, 15, 17, 21, 23] have made significant progress in exploiting subspace information for enhancing spectral clustering of highdimensional data, most of them tend to utilize a single affinity graph (associated with a single affinity matrix) by subspace learning, but lack the ability to go beyond a single affinity graph to jointly explore a variety of graph structures in various subspaces in the high-dimensional space. To overcome this limitation, this paper presents a new spectral clustering by subspace randomization and graph f usion (SC-SRGF) approach. Specifically, multiple random subspaces are first produced, based on which we construct multiple K-NN affinity graphs to capture the locality information in various subspaces. Then, the multiple affinity graphs (associated with multiple affinity matrices) are integrated into a unified affinity graph by using an iterative similarity network fusion scheme. With the unified graph obtained, the final spectral clustering result can be obtained by partitioning the this new affinity graph. We conduct experiments on twelve highdimensional datasets, which have shown the superiority of our approach.",
            "cite_spans": [
                {
                    "start": 23,
                    "end": 26,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 27,
                    "end": 29,
                    "text": "3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 30,
                    "end": 32,
                    "text": "4,",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 33,
                    "end": 36,
                    "text": "15,",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 37,
                    "end": 40,
                    "text": "17,",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 41,
                    "end": 44,
                    "text": "21,",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 45,
                    "end": 48,
                    "text": "23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "The rest of the paper is organized as follows. The proposed approach is described in Sect. 2. The experimental results are reported in Sect. 3. The paper is concluded in Sect. 4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this section, we describe the overall process of the proposed SC-SRGF approach. The formulation of the clustering problem is given in Sect. 2.1. The construction of multiple K-NN graphs (corresponding to multiple affinity matrices) in a variety of random subspaces is introduced in Sect. 2.2. Finally, the fusion of the multiple graphs into a unified graph and the spectral clustering process are described in Sect. 2.3.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Proposed Framework"
        },
        {
            "text": "Let X \u2208 R n\u00d7d be the data matrix, where n is the number of data points and d is the number of features. Let x i \u2208 R d denote the i-th data point, corresponding to the i-row in X. Thus the data matrix can be represented as X = (x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n ) . Let f j \u2208 R n denote the j-th data feature, corresponding to the j-th column in X. Thus the data matrix can also be represented as X = (f 1 , f 2 , \u00b7 \u00b7 \u00b7 , f d ). The purpose of clustering is to group the n data points into a certain number of subsets, each of which is referred to as a cluster.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Problem Formulation"
        },
        {
            "text": "In this work, we aim to enhance the spectral clustering for high-dimensional datasets with the help of the information of various subspaces. Before exploring the subspace information, a set of random subspaces are first generated. Note that each subspace consists of a certain number of features, and thereby corresponds to a certain number of columns in the data matrix X.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "Multiple random subspaces are generated by performing random sampling (without replacement) on the data features with a sampling ratio r. Let m denote the number of generated random subspaces. Then the set of random subspaces can be represented as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "where",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "denotes the i-th random subspace, f (i) j denotes the j-th feature in F (i) , and d = r \u00b7 d is the number of features. Each subspace can be viewed as selecting corresponding columns in the original data matrix. Therefore, the data submatrix in a given subspace F (i) can be represented as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "where x (i) j \u2208 R d denotes the j-th data point in this subspace.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "To explore the locality structures in various subspaces, multiple K-NN graphs are constructed. Specifically, given a subspace F (i) , its K-NN graph can be defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "where V = {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n } is the node set and E (i) is the edge set. The weights of the edges in the graph are computed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "where e",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "jk is the edge weight between nodes x j and",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "in the i-th subspace, and the kernel parameter \u03c3 is set to the average distance between all points.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "With the m random subspaces, we can construct m affinity graphs (corresponding to m affinity matrices) as follows:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "Note that these affinity graphs share the same node set (i.e., the set of all data points), but have different edge weights constructed in different subspaces, which enable them to capture a variety of underlying subspace structure information in high-dimensional space for enhanced clustering performance.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Affinity Construction in Random Subspaces"
        },
        {
            "text": "In this section, we proceed to fuse multiple affinity graphs (corresponding to multiple affinity matrices) into a unified affinity graph for robust spectral clustering of high-dimensional data. Specifically, we adopt the similarity network fusion (SNF) [20] scheme to fuse the information of multiple graphs. For simplicity, the set of the affinity matrices for the m graphs is represented as E = {E (1) , E (2) , \u00b7 \u00b7 \u00b7 , E (m) }. The goal here is to merge the m affinity matrices in E into a unified affinity matrix\u1ebc.",
            "cite_spans": [
                {
                    "start": 253,
                    "end": 257,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 400,
                    "end": 403,
                    "text": "(1)",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 408,
                    "end": 411,
                    "text": "(2)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "By normalizing the rows in the affinity matrix",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "And the kernel matrix",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "jk } n\u00d7n can be defined as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "With the above two types of matrices defined, we can iteratively update the status matrices by exploiting the information of multiple affinity matrices. Particularly, in each iteration, the i-th status matrix is updated as follows [20] :",
            "cite_spans": [
                {
                    "start": 231,
                    "end": 235,
                    "text": "[20]",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "After each iteration, P (i) t+1 will be normalized by P",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "being the degree matrix of P (i) t+1 . When the status matrices converge or the maximum number of iterations is reached, the iteration process stops and the fused affinity matrix will be computed as\u1ebc",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "Then the unified matrix\u1ebc will be symmetrized by\u1ebc = (\u1ebc +\u1ebc )/2. With the unified affinity matrix\u1ebc obtained by fusing information of multiple affinity matrices from multiple subspaces, we can proceed to perform spectral clustering on this unified matrix to build the clustering result with a certain number of, say, k , clusters.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "LetD be the degree matrix of\u1ebc. Its graph Laplacian can be computed as",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "After that, eigen-decomposition is performed on the graph LaplacianL to obtain the k eigenvectors that correspond to its first k eigenvalues. Then the k eigenvectors are stacked to form a new matrix\u0168 \u2208 R n\u00d7k , where the i-th column corresponds to the i-th eigenvector. Then, by treating each row as a new feature vector for the data point, some discretization techniques like k-means [18] can be performed on the matrix\u0168 to achieve the final spectral clustering result.",
            "cite_spans": [
                {
                    "start": 384,
                    "end": 388,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Fusing Affinity Graphs for Spectral Clustering"
        },
        {
            "text": "In this section, we conduct experiments on a variety of high-dimensional datasets to compare our approach against several other spectral clustering approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experiments"
        },
        {
            "text": "In our experiments, twelve real-world high-dimensional datasets are used, namely, Armstrong-2002-v1 [19] , Chowdary-2006 [19] , Golub-1999-v2 [19] , Alizadeh-2000-v2 [19] , Alizadeh-2000-v3 [19] , Bittner-2000 [19] , Bredel-2005 [19] , Garber-2001 [19] , Khan-2001 [19] , Binary-Alpha (BA) [14] , Coil20 [14] , and Multiple Features (MF ) [5] . To simplify the description, the twelve benchmark datasets are abbreviated as DS-1 to DS-12, respectively (as shown in Table 1 ). Table 2 .",
            "cite_spans": [
                {
                    "start": 100,
                    "end": 104,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 121,
                    "end": 125,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 142,
                    "end": 146,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 166,
                    "end": 170,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 190,
                    "end": 194,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 210,
                    "end": 214,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 229,
                    "end": 233,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 248,
                    "end": 252,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 265,
                    "end": 269,
                    "text": "[19]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 290,
                    "end": 294,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 304,
                    "end": 308,
                    "text": "[14]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 339,
                    "end": 342,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                }
            ],
            "ref_spans": [
                {
                    "start": 464,
                    "end": 471,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 475,
                    "end": 482,
                    "text": "Table 2",
                    "ref_id": null
                }
            ],
            "section": "Datasets and Evaluation Measures"
        },
        {
            "text": "To quantitatively evaluate the clustering results of different algorithms, two widely-used evaluation measures are used, namely, normalized mutual information (NMI) [7] and adjusted Rand index (ARI) [7] . Note that larger values of NMI and ARI indicate better clustering results.",
            "cite_spans": [
                {
                    "start": 165,
                    "end": 168,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 199,
                    "end": 202,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Datasets and Evaluation Measures"
        },
        {
            "text": "In terms of the experimental setting, we use m = 20, K = 5, and r = 0.5 on all the datasets in the experiments. In the following, the robustness of our approach with varying values of the parameters will also be evaluated in Sect. 3.3. Table 3 . Average ARI over 20 runs by different methods on the benchmark datasets. The best score in each row is in bold. Table 3 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 236,
                    "end": 243,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 358,
                    "end": 365,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "Datasets and Evaluation Measures"
        },
        {
            "text": "In this section, we compare the proposed SC-SRGF method against four baseline spectral clustering methods, namely, original spectral clustering (SC) [18] , k-means-based approximate spectral clustering (KASP) [22] , sparse subspace clustering (SSC) [3] , and sparse subspace clustering by orthogonal matching pursuit (SSC-OMP) [23] . The detailed comparison results are reported in Tables 2,  3 , and 4, and Figs. 1 and 2.",
            "cite_spans": [
                {
                    "start": 149,
                    "end": 153,
                    "text": "[18]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 209,
                    "end": 213,
                    "text": "[22]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 249,
                    "end": 252,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 327,
                    "end": 331,
                    "text": "[23]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 382,
                    "end": 394,
                    "text": "Tables 2,  3",
                    "ref_id": null
                }
            ],
            "section": "Comparison Against the Baseline Approaches"
        },
        {
            "text": "In terms of NMI, as shown in Table 2 , the proposed SC-SRGF method obtains the best scores on the DS-1, DS-3, DS-4, DS-5, DS-6, DS-9, DS-10, DS-11, and DS-12 datasets. The average NMI score (across the twelve datasets) of our method is 0.626, which is much higher than the second highest average score of 0.527 (obtained by SSC). The average rank of our method is 1.25, whereas the second best method only achieves an average rank of 3.00. As shown in Fig. 1 , our SC-SRGF method yields the best NMI scores on nine out of the twelve datasets in Table 2 , whereas the second and third best methods only achieves the best scores on two and one benchmark datasets, respectively. In terms of ARI, as shown in Table 3 , our SC-SRGF method also yields overall better performance than the baseline methods. Specifically, our method achieves an average ARI score (across twelve datasets) of 0.587, whereas the second best score is only 0.466. Our method obtains an average rank of 1.17, whereas the second best average rank is only 3.08. Further, as can be seen in Fig. 2 , our method achieves the best ARI score on ten out of the twelve datasets, which also significantly outperforms the other spectral clustering methods.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 29,
                    "end": 36,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 452,
                    "end": 458,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 545,
                    "end": 552,
                    "text": "Table 2",
                    "ref_id": null
                },
                {
                    "start": 705,
                    "end": 712,
                    "text": "Table 3",
                    "ref_id": null
                },
                {
                    "start": 1057,
                    "end": 1063,
                    "text": "Fig. 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Comparison Against the Baseline Approaches"
        },
        {
            "text": "In terms of time cost, as shown in Table 4 , it takes our SC-SRGF method less than 1 s to process the first nine smaller datasets and less than 30 s to process the other three larger datasets, which is comparable to the time costs of the SSC method. Therefore, with the experimental results in Tables 2, 3 , and 4 taken into account, it can be observed that our method is able to achieve significantly better clustering results for high-dimensional datasets (as shown in Tables 2 and  3 ) while exhibiting comparable efficiency with the important baseline of SSC (as shown in Table 4 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 35,
                    "end": 42,
                    "text": "Table 4",
                    "ref_id": null
                },
                {
                    "start": 294,
                    "end": 305,
                    "text": "Tables 2, 3",
                    "ref_id": null
                },
                {
                    "start": 471,
                    "end": 486,
                    "text": "Tables 2 and  3",
                    "ref_id": null
                },
                {
                    "start": 576,
                    "end": 583,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "Comparison Against the Baseline Approaches"
        },
        {
            "text": "All experiments were conducted in MATLAB R2016a on a PC with i5-8400 CPU and 64 GB of RAM.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Comparison Against the Baseline Approaches"
        },
        {
            "text": "In this section, we evaluate the performance of our SC-SRGF approach with three different parameters, i.e., the number of affinity matrices (or random subspaces) m, the number of nearest neighbors K, and the sampling ratio r. 10 20 Influence of the Number of Affinity Matrices m. The parameter m controls the number of random subspaces to be generated, which is also the number of affinity matrices to be fused in the affinity fusion process. Figure 3 illustrates the performance (w.r.t. NMI) of our SC-SRGF approach as the number of affinity matrices goes from 5 to 30 with an interval of 5. As shown in Fig. 3 , the performance of SC-SRGF is stable with different values of m. Empirically, a moderate value of m, say, in the interval of [10, 30] , is preferred. In the experiments, we use m = 20 on all of the datasets.",
            "cite_spans": [
                {
                    "start": 226,
                    "end": 228,
                    "text": "10",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 739,
                    "end": 743,
                    "text": "[10,",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 744,
                    "end": 747,
                    "text": "30]",
                    "ref_id": null
                }
            ],
            "ref_spans": [
                {
                    "start": 443,
                    "end": 451,
                    "text": "Figure 3",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 605,
                    "end": 611,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Parameter Analysis"
        },
        {
            "text": "Influence of the Number of Nearest Neighbors K. The parameter K controls the number of nearest neighbors when constructing the K-NN graphs for the multiple random subspaces. As can be seen in Fig. 4 , a smaller value of K can be beneficial to the performance, probably due to the fact that the K-NN graph with a smaller K may better reflect the locality characteristics in a given subspace. In the experiments, we use K = 5 on all of the datasets.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 192,
                    "end": 198,
                    "text": "Fig. 4",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Parameter Analysis"
        },
        {
            "text": "Influence of the Sampling Ratio r. The parameter r controls the sampling ratio when producing the multiple random subspaces from the high-dimensional space. As shown in Fig. 5 , a moderate value of r is often preferred on the benchmark datasets. Empirically, it is suggested that the sampling ratio be set in the interval of [0.2, 0.8].",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 169,
                    "end": 175,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Parameter Analysis"
        },
        {
            "text": "In the experiments, we use r = 0.5 on all of the datasets. Brief Summary. From the above experimental results, we can observe that the proposed SC-SRGF approach exhibits quite good consistency and robustness w.r.t. the three parameters, which do not require any sophisticated parameter tuning and can be safely set to some moderate values across different datasets. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter Analysis"
        },
        {
            "text": "In this paper, we propose a new spectral clustering approach termed SC-SRGF for high-dimensional data, which is able to explore diversified subspace information inherent in high-dimensional space by means of subspace randomization and affinity graph fusion. In particular, a set of multiple random subspaces are first generated by performing random sampling on the original feature space repeatedly. After that, multiple K-NN graphs are constructed to capture the locality information of the multiple subspaces. Then, we utilize an iterative graph fusion scheme to combine the multiple affinity graphs (i.e., multiple affinity matrices) into a unified affinity graph, based on which the final spectral clustering result can be achieved. We have conducted extensive experiments on twelve real-world high-dimensional datasets, which demonstrate the superiority of our SC-SRGF approach when compared with several baseline spectral clustering approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Spectral clustering of high-dimensional data via knearest neighbor based sparse representation coefficients",
            "authors": [
                {
                    "first": "F",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Fang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN)",
            "volume": "",
            "issn": "",
            "pages": "363--374",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Multi-view clustering in latent embedding space",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "S"
                    ],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "Proceedings of AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Sparse subspace clustering: algorithm, theory, and applications",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Elhamifar",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vidal",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "35",
            "issn": "11",
            "pages": "2765--2781",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Information theoretic subspace clustering",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Neural Netw. Learn. Syst",
            "volume": "27",
            "issn": "12",
            "pages": "2643--2655",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Locally weighted ensemble clustering",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lai",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Cybern",
            "volume": "48",
            "issn": "5",
            "pages": "1460--1473",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Ultra-scalable spectral clustering and ensemble clustering",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Wu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K"
                    ],
                    "last": "Kwoh",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/TKDE.2019.2903410"
                ]
            }
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Unsupervised feature selection with multisubspace randomization and collaboration. Knowl.-Based Syst",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "",
            "volume": "182",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Combining multiple clusterings via crowd agreement estimation and multi-granularity link analysis",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Neurocomputing",
            "volume": "170",
            "issn": "",
            "pages": "240--250",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Robust ensemble clustering using probability trajectories",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Knowl. Data Eng",
            "volume": "28",
            "issn": "5",
            "pages": "1312--1326",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Ensembling over-segmentations: from weak evidence to strong segmentation",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "H"
                    ],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [
                        "C"
                    ],
                    "last": "Yuen",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Neurocomputing",
            "volume": "207",
            "issn": "",
            "pages": "416--427",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Ensemble clustering using factor graph",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Pattern Recognit",
            "volume": "50",
            "issn": "",
            "pages": "131--142",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Enhanced ensemble clustering via fast propagation of cluster-wise similarities",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "K"
                    ],
                    "last": "Kwoh",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Syst. Man Cybern. Syst",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1109/TSMC.2018.2876202"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Data clustering: 50 years beyond k-means",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "K"
                    ],
                    "last": "Jain",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Pattern Recogn. Lett",
            "volume": "31",
            "issn": "8",
            "pages": "651--666",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Unified spectral clustering with optimal graph",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Kang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Proceedings of AAAI Conference on Artificial Intelligence",
            "volume": "",
            "issn": "",
            "pages": "3366--3373",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Robust subspace clustering by Cauchy loss function",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Dong",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Tao",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Neural Netw. Learn. Syst",
            "volume": "30",
            "issn": "7",
            "pages": "2067--2078",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Consistency meets inconsistency: a unified graph learning framework for multi-view clustering",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Liang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Proceedings of IEEE International Conference on Data Mining (ICDM)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Robust recovery of subspace structures by low-rank representation",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yu",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "volume": "35",
            "issn": "1",
            "pages": "171--184",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "A tutorial on spectral clustering",
            "authors": [
                {
                    "first": "U",
                    "middle": [],
                    "last": "Von Luxburg",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Stat. Comput",
            "volume": "17",
            "issn": "4",
            "pages": "395--416",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Clustering cancer gene expression data: a comparative study",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "C"
                    ],
                    "last": "De Souto",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "G"
                    ],
                    "last": "Costa",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "S"
                    ],
                    "last": "De Araujo",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "B"
                    ],
                    "last": "Ludermir",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Schliep",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "BMC Bioinformatics",
            "volume": "9",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Similarity network fusion for aggregating data types on a genomic scale",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Nat. Methods",
            "volume": "11",
            "issn": "",
            "pages": "333--337",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Provable subspace clustering: when LRR meets SSC",
            "authors": [
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Xu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Leng",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Trans. Inf. Theory",
            "volume": "65",
            "issn": "9",
            "pages": "5406--5432",
            "other_ids": {}
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Fast approximate spectral clustering",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "I"
                    ],
                    "last": "Jordan",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
            "volume": "",
            "issn": "",
            "pages": "907--916",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Scalable sparse subspace clustering by orthogonal matching pursuit",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "You",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "P"
                    ],
                    "last": "Robinson",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Vidal",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "One-step kernel multiview subspace clustering. Knowl.-Based Syst",
            "authors": [
                {
                    "first": "G",
                    "middle": [
                        "Y"
                    ],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "R"
                    ],
                    "last": "Zhou",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [
                        "Y"
                    ],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                }
            ],
            "year": 2020,
            "venue": "",
            "volume": "189",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "366\u00b10.000 0.263\u00b10.104 0.366\u00b10.000 0.351\u00b10.000 0.546\u00b10.117 DS-2 0.081\u00b10.000 0.171\u00b10.295 0.764\u00b10.000 0.860\u00b10.000 0.849\u00b10.022 DS-3 0.596\u00b10.000 0.404\u00b10.245 0.690\u00b10.000 0.700\u00b10.000 0.801\u00b10.049 DS-4 0.605\u00b10.000 0.851\u00b10.164 0.734\u00b10.000 0.620\u00b10.000 0.913\u00b10.000 DS-5 0.560\u00b10.000 0.614\u00b10.061 0.442\u00b10.001 0.441\u00b10.007 0.626\u00b10.002 DS-6 0.032\u00b10.000 0.032\u00b10.027 0.035\u00b10.000 0.035\u00b10.000 0.053\u00b10.003 DS-7 0.249\u00b10.000 0.367\u00b10.089 0.102\u00b10.000 0.115\u00b10.000 0.311\u00b10.075 DS-8 0.082\u00b10.005 0.139\u00b10.055 0.086\u00b10.004 0.172\u00b10.011 0.161\u00b10.024 DS-9 0.604\u00b10.000 0.328\u00b10.073 0.835\u00b10.000 0.533\u00b10.009 0.881\u00b10.014 DS-10 0.503\u00b10.005 0.591\u00b10.009 0.580\u00b10.006 0.260\u00b10.006 0.613\u00b10.007 DS-11 0.780\u00b10.000 0.860\u00b10.022 0.864\u00b10.005 0.517\u00b10.201 0.888\u00b10.001 DS-12 0.655\u00b10.000 0.866\u00b10.018 0.824\u00b10.001 0.556\u00b10.002 0.871\u00b10.Number of times being ranked in the first position in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": ".000 0.152\u00b10.058 0.268\u00b10.000 0.238\u00b10.000 0.578\u00b10.181 DS-2 0.066\u00b10.000 0.168\u00b10.340 0.851\u00b10.000 0.924\u00b10.000 0.916\u00b10.015 DS-3 0.656\u00b10.000 0.378\u00b10.270 0.707\u00b10.000 0.729\u00b10.000 0.844\u00b10.047 DS-4 0.506\u00b10.000 0.897\u00b10.148 0.796\u00b10.000 0.627\u00b10.000 0.947\u00b10.000 DS-5 0.360\u00b10.003 0.479\u00b10.057 0.261\u00b10.006 0.289\u00b10.005 0.427\u00b10.005 DS-6 0.018\u00b10.000 0.009\u00b10.029 0.020\u00b10.000 0.020\u00b10.000 0.047\u00b10.036 DS-7 0.277\u00b10.000 0.387\u00b10.169 0.105\u00b10.000 0.112\u00b10.000 0.404\u00b10.122 DS-8 0.068\u00b10.010 0.059\u00b10.067 0.0004\u00b10.003 0.103\u00b10.020 0.128\u00b10.024 DS-9 0.466\u00b10.000 0.206\u00b10.056 0.826\u00b10.000 0.433\u00b10.009 0.860\u00b10.011 DS-10 0.210\u00b10.005 0.291\u00b10.011 0.300\u00b10.008 0.051\u00b10.004 0.327\u00b10.008 DS-11 0.638\u00b10.000 0.682\u00b10.055 0.701\u00b10.017 0.260\u00b10.019 0.744\u00b10.002 DS-12 0.559\u00b10.000 0.818\u00b10.029 0.754\u00b10.000 0.445\u00b10.006 0.826\u00b10.056 Number of times being ranked in the first position in",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Average time costs (s) by different methods on the benchmark datasets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Average NMI over 20 runs by SC-SRGF with varying number of affinity matrices m.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Average NMI over 20 runs by SC-SRGF with varying number of nearest neighbors K.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Average NMI over 20 runs by SC-SRGF with varying sampling ratio r.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Dataset description Table 2. Average NMI over 20 runs by different methods on the benchmark datasets. The best score in each row is in bold.",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "A*STAR-NTU-SUTD AI Partnership Grant (No. RGANS1905).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Acknowledgments. This work was supported by NSFC (61976097 & 61876193) and"
        }
    ]
}