{
    "paper_id": "f42c7f34a7ef3435167de83ba15adb90f700c008",
    "metadata": {
        "title": "Motion Words: A Text-Like Representation of 3D Skeleton Sequences",
        "authors": [
            {
                "first": "Jan",
                "middle": [],
                "last": "Sedmidubsky",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Masaryk University",
                    "location": {
                        "settlement": "Brno",
                        "region": "Czechia"
                    }
                },
                "email": ""
            },
            {
                "first": "Petra",
                "middle": [],
                "last": "Budikova",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Masaryk University",
                    "location": {
                        "settlement": "Brno",
                        "region": "Czechia"
                    }
                },
                "email": "budikova@fi.muni.cz"
            },
            {
                "first": "Vlastislav",
                "middle": [],
                "last": "Dohnal",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Masaryk University",
                    "location": {
                        "settlement": "Brno",
                        "region": "Czechia"
                    }
                },
                "email": "dohnal@fi.muni.cz"
            },
            {
                "first": "Pavel",
                "middle": [],
                "last": "Zezula",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Masaryk University",
                    "location": {
                        "settlement": "Brno",
                        "region": "Czechia"
                    }
                },
                "email": "zezula@fi.muni.cz"
            }
        ]
    },
    "abstract": [
        {
            "text": "There is a growing amount of human motion data captured as a continuous 3D skeleton sequence without any information about its semantic partitioning. To make such unsegmented and unlabeled data efficiently accessible, we propose to transform them into a text-like representation and employ well-known text retrieval models. Specifically, we partition each motion synthetically into a sequence of short segments and quantize the segments into motion words, i.e. compact features with similar characteristics as words in text documents. We introduce several quantization techniques for building motion-word vocabularies and propose application-independent criteria for assessing the vocabulary quality. We verify these criteria on two real-life application scenarios.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "In recent years, we have witnessed a rapid development of motion capture devices and 3D pose-estimation methods [2] that enable recording human movements as a sequence of poses. Each pose keeps the 3D coordinates of important skeleton joints in a specific time moment. Effective and efficient processing of such spatio-temporal data is very desirable in many application domains, ranging from computer animation, through sports and medicine, to security [5, 7, 9] .",
            "cite_spans": [
                {
                    "start": 112,
                    "end": 115,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 454,
                    "end": 457,
                    "text": "[5,",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 458,
                    "end": 460,
                    "text": "7,",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 461,
                    "end": 463,
                    "text": "9]",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "To illustrate the range of possible tasks over motion data, let us assume that we have the 3D skeleton data from a figure skating competition. Existing research mainly focuses on action recognition [23] , i.e. categorizing the figure performed in a given, manually selected motion segment. This is typically solved using convolutional [1, 17] or recurrent [10, 20, 22] neural-network classifiers. However, this approach is not applicable to other situations where motion data are captured as long continuous sequences without explicit knowledge of semantic partitioning. In such cases, other techniques need to be applied, e.g., subsequence search to find all competitors who performed the triple Axel jump, or similarity joins to identify different performances of the same choreography, similar choreographies, or the most common figures. These techniques require identifying query-relevant subsequences within the continuous motion data. To allow efficient evaluation of such queries, the data need to be automatically segmented and indexed.",
            "cite_spans": [
                {
                    "start": 198,
                    "end": 202,
                    "text": "[23]",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 335,
                    "end": 338,
                    "text": "[1,",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 339,
                    "end": 342,
                    "text": "17]",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 356,
                    "end": 360,
                    "text": "[10,",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 361,
                    "end": 364,
                    "text": "20,",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 365,
                    "end": 368,
                    "text": "22]",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Since a universal semantic segmentation is hardly achievable, we suggest to partition each motion sequence synthetically into short fixed-size segments whose length is smaller than the expected size of future queries. In this way, we transform the input motion into an ordered sequence of segments, structurally similar to a text document. To complete the analogy, we quantize the segments into compact representations, denoted as motion words (MWs), having similar properties as words in text documents. Individual MWs deal with the spatial variability of the short segments, whereas the temporal variability of longer motions is captured by the MW order and quantified by mature text-retrieval models [12] . We believe that such universal text-based representation is applicable for a wide range of applications that need to process continuous motion data efficiently, as illustrated in Fig. 1 .",
            "cite_spans": [
                {
                    "start": 703,
                    "end": 707,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [
                {
                    "start": 889,
                    "end": 895,
                    "text": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ],
            "section": "Introduction"
        },
        {
            "text": "In this paper, we mainly focus on effective quantization of the motion segments to build a vocabulary of motion words. The most desirable MW property is that two MWs match each other if their corresponding segments exhibit similar movement characteristics, and do not match if the segments are dissimilar. This is challenging with the quantization approach, since it is in general not possible to divide a given space in such way that all pairs of similar objects are in the same partition. Some pairs of similar segments thus get separated by partition borders and become non-matching, which we denote as the border problem. We answer this challenge by designing two MW construction techniques that reduce the border problem but still enable efficient organization using text retrieval techniques. Furthermore, we recommend generic (application-independent) criteria for selection of a suitable vocabulary for specific application needs, and verify the usability of such criteria on two real-life applications.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Most existing works that process continuous 3D skeleton sequences in an unsupervised way focus on subsequence search [18] , unsupervised segmentation [8] , or anticipating future actions based on the past-to-current data [4] . In [18] , the continuous sequences are synthetically partitioned into a lot of overlapping and variable-size segments that are represented by 4, 096D deep features. However, indexing a large number of such very high-dimensional features is costly. To move towards more efficient processing, the approaches in [3, 11] quantize the segment features using a single k-means clustering. However, with such simple quantization the border problem appears frequently, which decreases the effectiveness of applications with an increasing number of clusters (i.e. the vocabulary size).",
            "cite_spans": [
                {
                    "start": 117,
                    "end": 121,
                    "text": "[18]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 150,
                    "end": 153,
                    "text": "[8]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 221,
                    "end": 224,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 230,
                    "end": 234,
                    "text": "[18]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 536,
                    "end": 539,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 540,
                    "end": 543,
                    "text": "11]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Related Work and Our Contributions"
        },
        {
            "text": "In our research, we also take inspiration from image processing where highdimensional image features are quantized into visual words. There are two lines of research that are important to us: fundamental quantization techniques, and reducing the border problem. The image quantization strategies have evolved from basic k-means clustering used in [21] , through cluster hierarchies [14] , approximate k-means [16] , to recent deep neural-network approaches [24] . The influence of the border problem can be reduced using a weighted combination of the nearest visual words for each feature [16] , or by a consensus voting of multiple independent vocabularies [6] .",
            "cite_spans": [
                {
                    "start": 347,
                    "end": 351,
                    "text": "[21]",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 382,
                    "end": 386,
                    "text": "[14]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 409,
                    "end": 413,
                    "text": "[16]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 457,
                    "end": 461,
                    "text": "[24]",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 589,
                    "end": 593,
                    "text": "[16]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 658,
                    "end": 661,
                    "text": "[6]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Related Work and Our Contributions"
        },
        {
            "text": "We propose an effective quantization of unlabeled 3D skeleton data into sequences of motion words that can be efficiently managed by text-retrieval techniques. In contrast to previous works, we give a particular attention to the border problem. Specifically, -we systematically analyze the process of MW vocabulary construction and discuss possible solutions of the border problem (Sect. 3); -we propose application-independent criteria that do not require labeled data for selecting a suitable MW vocabulary for a given task (Sect. 3.3); -we implement three vocabulary construction techniques that differ in dealing with the border problem, and evaluate their quality (Sect. 4); -we verify the suitability of the proposed criteria by evaluating the best-ranked vocabularies in the context of two real-world applications (Sect. 5).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Contributions of This Paper"
        },
        {
            "text": "The motion-word (MW) approach assumes that the continuous 3D skeleton data are cut into short, possibly overlapping segments which are consequently transformed into the motion words. The segment and overlap lengths are important parameters of the whole system and have also been studied in our experiments, however their thorough analysis is out of the scope of this paper. Therefore, we assume that a suitable segmentation is available, and focus solely on transforming the segment space into the space of motion words, denoted as the MW vocabulary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MW Vocabulary Construction"
        },
        {
            "text": "The MW vocabulary consists of a finite set of motion words and a Booleanvalued MW matching function that determines whether two MWs are consid- is a standard text-processing primitive required by most text retrieval techniques [12] . The transformation from segments to MWs has to be similaritypreserving: with a high probability, similar segment pairs need to be mapped to matching MWs and dissimilar segment pairs to non-matching MWs. Noticeably, the vocabulary construction can be investigated independently of a particular application, since it only considers the distribution of segments in the segment space. We propose to build the MW vocabulary using quantization of the segment space, which can be seen as analogous to the word stemming in text processing.",
            "cite_spans": [
                {
                    "start": 227,
                    "end": 231,
                    "text": "[12]",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": "MW Vocabulary Construction"
        },
        {
            "text": "In the following, we first review the standard quantization approach that leads to a basic MW model and discuss its limitations, namely the border problem. Next, we introduce a generalized MW model with two techniques for reducing the border problem. Lastly, we present the evaluation methodology that we propose for comparing the quality of different vocabularies.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MW Vocabulary Construction"
        },
        {
            "text": "Basic data quantization is usually performed by the k-means algorithm that divides the segment space into non-overlapping partitions [3, 11, 21] . Each partition can be assigned a one-dimensional identifier, which constitutes the motion word. Each motion segment is associated with exactly one MW, which we denote as the hard quantization (Fig. 2a) . To compare two hard MWs, a trivial MW matching function is defined: it returns 1 for identical words and 0 otherwise.",
            "cite_spans": [
                {
                    "start": 133,
                    "end": 136,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 137,
                    "end": 140,
                    "text": "11,",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 141,
                    "end": 144,
                    "text": "21]",
                    "ref_id": "BIBREF22"
                }
            ],
            "ref_spans": [
                {
                    "start": 339,
                    "end": 348,
                    "text": "(Fig. 2a)",
                    "ref_id": null
                }
            ],
            "section": "Basic MW Model"
        },
        {
            "text": "Using this approach, the 3D skeleton data are transformed into a sequence of scalar MWs to be readily processed by the standard text-retrieval tools. However, the hard quantization makes it difficult to preserve the similarity between the segments. Unless the input data are inherently well-clustered, which is not likely in the high-dimensional segment space, it is not possible to avoid the border problem, i.e. the situations when two similar segments get assigned to different MWs (s 1 and s 2 in Fig. 2a) . Moreover, finding a good clustering is computationally expensive. Therefore, approximate or sampling methods are often used for large data, which makes the border problem even more pronounced.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 501,
                    "end": 509,
                    "text": "Fig. 2a)",
                    "ref_id": null
                }
            ],
            "section": "Basic MW Model"
        },
        {
            "text": "We believe that the border problem can be reduced significantly if we allow a given segment to be associated with several partitions of the input space. Therefore, we define the generalized MW as a collection of MW elements, where each element corresponds to a single partition of the input space. In contrast to the basic model where individual MWs are atomic and mutually exclusive, the generalized MWs may share some MW elements. This allows us to define a more fine-grained MW matching function that better approximates the original similarity between the motion segments.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalized MW Model"
        },
        {
            "text": "As illustrated in Fig. 2b and c, we adopt the following two orthogonal principles of selecting the MW elements for a given segment.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 18,
                    "end": 25,
                    "text": "Fig. 2b",
                    "ref_id": null
                }
            ],
            "section": "Generalized MW Model"
        },
        {
            "text": "-Soft quantization. Recall again that the border problem occurs when two similar segments are separated into different partitions. Intuitively, at least one of these segments has to lie near the partition border. Segment s 1 in Fig. 2a lies outside the partition D but is close to its borders, so there is a good chance that some segments similar to s 1 are in D. Therefore, it could be helpful to associate s 1 also with D. Following this idea, we define the soft MW for s 1 as an ordered set of one or more MW elements, where the first base element identifies the partition containing s 1 and the remaining expanded elements refer to the partitions that are sufficiently close to s 1 (see Fig. 2b for illustration). A naive MW matching function could return 1 whenever the intersection of two soft MWs is non-empty, however this tends to match even segments that are not so close in the segment space (s 1 and s 3 in Fig. 2b) . Therefore, our soft-base matching function returns 1 only if the intersection contains at least one base element. -Multi-overlay quantization: So far, we have assumed that the MW elements are taken from a single partitioning of the segment space. However, it is also possible to employ several independent partitioning overlays obtained by different methods. A single overlay may incorrectly separate a pair of similar segments, but it is less probable that the same pair will be separated by the other independent overlays. We define the multi-overlay MW as an n-tuple of MW elements that are assigned to a given segment in the individual overlays.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 228,
                    "end": 235,
                    "text": "Fig. 2a",
                    "ref_id": null
                },
                {
                    "start": 691,
                    "end": 698,
                    "text": "Fig. 2b",
                    "ref_id": null
                },
                {
                    "start": 919,
                    "end": 927,
                    "text": "Fig. 2b)",
                    "ref_id": null
                }
            ],
            "section": "Generalized MW Model"
        },
        {
            "text": "To decide whether two MWs match, the consensus of m out of n MW elements is used. The matching function returns 1 if the multi-overlay MWs agree on at least m positions of the respective n-tuples (see Fig. 2c ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 201,
                    "end": 208,
                    "text": "Fig. 2c",
                    "ref_id": null
                }
            ],
            "section": "Generalized MW Model"
        },
        {
            "text": "By allowing the MWs to be compound, we improve the quantization quality but create new challenges regarding indexability. The generalized MWs are no longer scalar and cannot be simply treated the same way as text words. However, existing text retrieval tools can be adjusted to index both the soft and multioverlay MWs, as briefly discussed in Sect. 4.4.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Generalized MW Model"
        },
        {
            "text": "For evaluating MW vocabularies, we need to consider two different aspects: (i) vocabulary quality -measured by the application-independent ability to perform a similarity-preserving transformation from the segment space to the MW space, and (ii) vocabulary usefulness -measured by effectiveness of the application employing the specific vocabulary. Our objective is to show that both vocabulary quality and vocabulary usefulness are related, so we can choose a suitable vocabulary without evaluating it within the real application, i.e. not needing the application ground truth (GT).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Methodology"
        },
        {
            "text": "In the following, we introduce the dataset used for both types of evaluation, and describe the application-independent vocabulary quality measures that are examined in Sect. 4. The vocabulary usefulness is discussed in Sect. 5.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Methodology"
        },
        {
            "text": "We adopt the HDM05 dataset [13] of 3D skeleton sequences, which consists of 2, 345 labeled actions categorized in 130 classes. The actions capture exercising and daily movement activities with the sampling frequency of 120 Hz and track 31 skeleton joints. The action length ranges from 13 frames (108 ms) to 900 frames (7.5 s). We use this dataset to evaluate the MW usefulness in two applications: a kNN classification of actions, and a similar action search. These applications do not require complex retrieval algorithms and allow us to clearly show the effect of MWs on application effectiveness.",
            "cite_spans": [
                {
                    "start": 27,
                    "end": 31,
                    "text": "[13]",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "Both the vocabulary construction and the application-independent quality assessment are designed for completely unlabeled segment data, which we extract from the HDM05 dataset as follows. We divide each action synthetically into a sequence of overlapping segments. As recommended in [3] , we fix the segment length to 80 frames and the segment overlap to 64 frames, so the segments are shifted by 16 frames. This generates 28 k segments in total, with 12 segments per action on average. We also down-sample the segments to 12 frames per second. The similarity of any two segments is determined by the Dynamic Time Warping (DTW), where the pose distance inside DTW is computed as the sum of Euclidean distances between the 3D coordinates of the corresponding joints.",
            "cite_spans": [
                {
                    "start": 283,
                    "end": 286,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "Estimating GT for Unlabeled Segments. The similarity-preserving property states that similar segments should be mapped to matching MWs, whereas dissimilar segments to non-matching MWs. To be able to check this property for a given vocabulary, we need a ground truth (GT) of similar and dissimilar segment pairs. Since the segments have no semantic labels, we can only use pairwise distances to estimate the GT. Using the distance distribution of all segments from our dataset, we determine two threshold distances that divide the segment pairs into similar pairs, dissimilar pairs, and a grey zone. In particular, the 0.5 th percentile distance becomes the similarity threshold T sim and all segment pairs with the mutual distance lower than T sim are the GT's similar pairs. The 40 th percentile becomes the dissimilarity threshold T dissim which defines the dissimilar pairs. Both the thresholds are set tightly to eliminate the chance that semantically unrelated segments are considered similar and vice versa. The segment pairs with mutual distance between T sim and T dissim form the grey zone and are ignored in the vocabulary quality evaluations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Dataset."
        },
        {
            "text": "To assess how well a given MW vocabulary manages to match a given segment with similar segments, we use standard IR measures of precision (P ) and recall (R) computed over the above-described GT of similar and dissimilar segment pairs: P = tp tp+fp and R = tp tp+fn , where the true positives (tp) are pairs of similar segments mapped to matching MWs, false positives (fp) are dissimilar segments with matching MWs, etc. To quantify the trade-off between P and R, we employ the F \u03b2 score = (1 + \u03b2 2 ) \u00b7 P \u00b7R (\u03b2 2 \u00b7R)+P , where the positive real \u03b2 is used to adjust the importance of the precision and recall according to the target application preferences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Vocabulary Quality Measures."
        },
        {
            "text": "As already mentioned, we test our vocabularies in context of two applications with different needs. The kNN classification requires high precision of retrieved actions for correct decision, but some positives can be missed. On the other hand, action search typically requires high recall. With these two applications in mind, we select the following two F scores for our experiments: F 0.25 score that emphasizes precision, as required by the classification task, and F 1 score that is the harmonic mean of both precision and recall and complies to the needs of a search-oriented application.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Vocabulary Quality Measures."
        },
        {
            "text": "To create a vocabulary, we use a Voronoi partitioning of the segment space. It assumes a set of sites (pivots) is selected beforehand by a particular selection algorithm. The Voronoi cell of pivot p is formed by all segments closer to p than to the other pivots. The pivots' IDs become the motion words or MW elements. Regarding the pivot selection, we must keep in mind that the segment space may not be the Euclidean space, which is our case with DTW that brakes the triangle inequality. So a particular pivot selection algorithm must respect that an artificial data item (e.g., a mean vector) cannot be computed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation and Evaluation"
        },
        {
            "text": "In the following, we introduce algorithms implementing the aforementioned MW vocabulary construction principles, and show how the quality measures introduced in Sect. 3.3 can be used to tune the parameters of the algorithms.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation and Evaluation"
        },
        {
            "text": "Firstly, we analyze the viability of three pivot selection techniques: the kmedoids, the hierarchical k-medoids, and a random selection. We also study the influence of the number of pivots, which determines the cardinality of the vocabulary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hard Quantization"
        },
        {
            "text": "Implementation. The k-medoids algorithm is a variation of the k-means clustering that is mostly used for quantization. It works in iterations, gradually moving from a random set of pivots to more optimal ones. With the k-medoids, the pivots must be selected from existing motion segments. The optimization criterion is to minimize the sum of distances to other segments within the cluster. The algorithm does not guarantee to find the global optimum and is very costly since the distances of all pivot-object pairs need to be computed in each iteration. The hierarchical k-medoids (hk-medoids) seeks the pivots by recursive application of k-medoids, which allows using much smaller values of k in each iteration to create a vocabulary of the same size. The pivots for the next level are always selected from the parental cell, so the data locality is preserved. We use a constant number of pivots per level and similar pivot numbers across levels. For example, the set-up 39|38 denotes 39 pivots in the root level and 38 pivots in the second level, which creates 1,482 cells. Finally, we also try a random selection of pivots where a pivot too close to another one is omitted. This is the most efficient approach which is known to perform well in permutation-based indexes [15] .",
            "cite_spans": [
                {
                    "start": 1273,
                    "end": 1277,
                    "text": "[15]",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": "Hard Quantization"
        },
        {
            "text": "Experimental Evaluation. Using the three algorithms, we create vocabularies of sizes ranging from 100 up to 3,000 MWs, and compare their quality. The results presented in Fig. 3 are averages over five runs. In general, the higher the precision is the more pivots are used, and vice-versa for the recall. A good vocabulary is prepared by techniques choosing the pivots in correspondence to the distribution of segments, thus the random selection should be rejected, since its precision is low. Focusing on the vocabulary size, the F 0.25 score that favors precision guides us to pick the k-medoids with 350 or 500 pivots and the hk-medoids of the 32|31 breakdown. In the F 1 score, the optimum is 100 or 350 pivots by k-medoids, and 19|18 or 10|10|10 by hk-medoids.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 171,
                    "end": 177,
                    "text": "Fig. 3",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Hard Quantization"
        },
        {
            "text": "The k-medoids with 350 pivots has been identified as the most promising hard quantization method, therefore we use it in the following trials exclusively. We also experimented with the best settings of the other algorithms and obtained analogous trends, so we do not include them.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Hard Quantization"
        },
        {
            "text": "Secondly, it is vital for the soft quantization to assign additional MW elements of neighboring cells only to the segments that are close to the cell borders. We limit such closeness by the distance D and bound the number of MW elements to the maximum number K. We study the influence of D and K on the quality measures, which should show that the border problem is reduced. We gradually check all pivots p i and expand the segment's MW with the MW element p i until the estimated distance exceeds D. The value of D must be smaller than the similarity threshold T sim discussed in Sect. 3.3, since it identifies objects that should be assigned the same MW. There can be many neighboring cells, so we constrain the MW elements to the K closest ones. Experimental Evaluation. We vary the values of D from 10 ( 1 /8 \u00b7 T sim ) to 80 (T sim ), and K from 2 to 6. The relevant results are shown in Fig. 4a . Increasing K for a small D (D10, K2-6) leads to improved recall and nearly constant precision. On the other hand, multiplying D (D10-80, K6) produces extensive MWs, which reduces the border problem (recall is boosted), but it negatively effects precision. For the classification task (F 0.25 score), D10, K6 and D20, K6 are the best, while D40, K6 is the optimum for the search (F 1 score).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 892,
                    "end": 899,
                    "text": "Fig. 4a",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "Soft Quantization"
        },
        {
            "text": "Thirdly, independent sets of pivots are likely to provide different Voronoi partitionings, thus increasing a chance of similar segments to share the same cell. We create up to 5 overlays and vary the number of overlays required to agree.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Multi-overlay Quantization"
        },
        {
            "text": "Since the k-medoids algorithm provides a locally optimal solution, we ran it five times to obtain different sets of 350 pivots for the multioverlay quantization. Noticeably, the quality of hard vocabularies created from individual sets of pivots differs up to 5% in both the F scores.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Implementation."
        },
        {
            "text": "Experimental Evaluation. In Fig. 4b , we present the results for all combinations of the five overlays, where the notation m/n refers to the m-out-of-n matching function. The combination 1/1 corresponds to hard quantization. When we fix m to 1 and add more overlays, the border problem is reduced, as witnessed by a major improvement of recall and only a marginal drop in precision. Similar trends can be observed also for higher values of m, but the actual values of recall get lower when we require more overlays to agree. The most restrictive combination 5/5 requires all overlays to meet and performs similarly to the hard quantization with more than 3,000 pivots (see Fig. 3a ). The best F 0.25 score is for the 2/5 setup and the best F 1 score is for the 1/5 setting.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 28,
                    "end": 35,
                    "text": "Fig. 4b",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 673,
                    "end": 680,
                    "text": "Fig. 3a",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "Implementation."
        },
        {
            "text": "By thorough experimentation, we have observed that the k-medoids clustering is the best hard quantization method but its quality can still be significantly improved by the soft and multi-overlay principles. The suppression of the border problem is mainly attributed to the increased number of correctly matched segment pairs (true positives) by both these principles. Although some new false positives are introduced, they decrease the overall precision only marginally.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Since the k-medoids clustering has high computation complexity, we have also considered cheaper techniques, i.e. the random clustering, with the soft and multi-overlay approach. However, the experimental results were not much competitive, so the k-medoids still remains a reasonable choice for quantization.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Our vocabulary construction techniques are universal, but the created vocabulary is clearly data-dependent. Since our evaluation data are relatively small (28,104 segments), the optimal vocabulary size is 350 MWs for the hard quantization. For larger and more diverse data, we expect the quality measures to recommend a larger vocabulary.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "Finally, a successful application also requires fast access to the data, which calls for indexes. The hard vocabulary can be directly organized in an inverted file. The soft-assigned vocabulary just expands the query, so the inverted file is sought multiple times (proportional to the number of MW elements in the query). The multi-overlay vocabulary can be managed in separate search indexes (one per overlay) and the query results merged to compute the m-out-of-n matching.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "In this section, we experimentally verify that: (i) the MW representation preserves important characteristics of complex 3D skeleton data and causes no drop in application effectiveness (Sect. 5.2), and (ii) the vocabulary quality measures well approximate the usefulness of different vocabularies in applications. Both these aspects are evaluated in context of two applications: the action classification that aims at recognizing the correct class of a given action using a kNN classifier, and the action search where the goal is to retrieve all actions relevant to a query, i.e. the actions belonging to the same class as the query.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Motion Words in Applications"
        },
        {
            "text": "The input for both classification and search applications is the dataset of 2, 345 synthetically-segmented actions discussed in Sect. 3.3. On average, each action is transformed into a sequence of 12 MWs. To compare two MW sequences, we again adopt the DTW sequence alignment function. Realize that the MW matching function inside DTW deals with the spatial variability of short segments, whereas DTW considers the temporal dimension of the whole actions.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Evaluation Methodology of Classification/Search Applications"
        },
        {
            "text": "Both applications are evaluated on the basis of k-nearest neighbor (kNN) queries. We use the standard leave-one-out approach to evaluate 2, 345 kNN queries in a sequential way by computing the distance between the specific query action and each of the remaining dataset actions. For the classification task, we fix k to 4 and apply a 4NN classifier (similar to the classifier proposed in [19] ). We measure the application effectiveness as the average classification accuracy over all 2, 345 queries. For the search task, the value of k is adjusted for each query individually based on the number of available actions belonging to the same class as the query action. Such adaptive value of k allows us to focus on recall as well as precision. The effectiveness of the search application is then determined as the average recall over all the queries. Note that the recall is always the same as the precision in the search task with the adaptive value of k.",
            "cite_spans": [
                {
                    "start": 388,
                    "end": 392,
                    "text": "[19]",
                    "ref_id": "BIBREF20"
                }
            ],
            "ref_spans": [],
            "section": "Evaluation Methodology of Classification/Search Applications"
        },
        {
            "text": "We quantify the usefulness of the MW concept by evaluating application effectiveness with different vocabularies and comparing it to the baseline case that uses no quantization (i.e. the action segments are represented by original 3D skeleton data). The most interesting results are summarized in Table 1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 297,
                    "end": 304,
                    "text": "Table 1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Usefulness and Efficiency of MWs"
        },
        {
            "text": "For classification, we observe that the baseline case achieves the effectiveness of 77.70%. Worse results have been expected for any MW quantization due to the dimensionality reduction of the original segment data. A standard hard quantization -the single-level k-medoids -indeed achieves the worst result (74.97%). Surprisingly, the soft-assignment D10, K6 vocabulary reaches basically the same effectiveness (77.61%) as the baseline, and the 2/5 multi-overlay quantization is actually better (80.30%). Thus, the best MW vocabulary not only preserves important motion characteristics but also aggregates many tiny variations in joint positions that confuse DTW on raw 3D skeleton data (the baseline case).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Usefulness and Efficiency of MWs"
        },
        {
            "text": "A similar trend can be observed on the search application where the hard quantization has the worst result too. As the recall is very important for the search task, the 1/5 multi-overlay vocabulary is now the best candidate that also outperforms the baseline case. Compared to the state-of-the art approaches [3, 11] that employ the hard quantization, the proposed generalized MWs reach much better effectiveness, e.g., about 25% higher recall in the search application (increase from 44.21% to 55.62%).",
            "cite_spans": [
                {
                    "start": 309,
                    "end": 312,
                    "text": "[3,",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 313,
                    "end": 316,
                    "text": "11]",
                    "ref_id": "BIBREF11"
                }
            ],
            "ref_spans": [],
            "section": "Usefulness and Efficiency of MWs"
        },
        {
            "text": "From the performance point of view, it takes almost 1.5 h to evaluate all the 2, 345 kNN queries with the baseline segment representation. Using any of the MW representations, the evaluation finishes in 30 s, which is an improvement by two orders of magnitude.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Usefulness and Efficiency of MWs"
        },
        {
            "text": "Remember that in Sect. 3.3 we proposed to quantify the vocabulary quality by the F \u03b2 score, where the parameter \u03b2 is set according to the precision/recall preference of the target application. For classification and search, we proposed to use F 0.25 and F 1 , respectively. Here, we verify whether such F \u03b2 scores correspond to the actual usefulness of individual vocabularies. To do so, we apply the vocabularies discussed in Sects. 4.1, 4.2 and 4.3 to our real-life applications and measure the application effectiveness.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Concordance of Vocabulary Quality and Usefulness"
        },
        {
            "text": "The results in Fig. 5 confirm that the estimated quality of vocabularies (red line in Fig. 5a and yellow line in Fig. 5b) shares the same trend with the actual vocabulary usefulness measured by the real classification (grey dashed line) and search (grey solid line) effectiveness. Therefore, the F \u03b2 score can be used for selecting the most suitable vocabulary for a given application, instead of a tedious and costly experimenting with all candidate vocabularies within the application. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 15,
                    "end": 21,
                    "text": "Fig. 5",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 86,
                    "end": 93,
                    "text": "Fig. 5a",
                    "ref_id": "FIGREF5"
                },
                {
                    "start": 113,
                    "end": 121,
                    "text": "Fig. 5b)",
                    "ref_id": "FIGREF5"
                }
            ],
            "section": "Concordance of Vocabulary Quality and Usefulness"
        },
        {
            "text": "This paper studies the possibility of transforming unlabeled 3D skeleton data into text-like representations that allow efficient processing. In particular, we focused on quantizing short synthetic motion segments into compact, similaritypreserving motion words (MWs). In contrast to existing works on motion quantization, we recognize the border problem and try to minimize it using the soft-assignment and multi-overlay partitioning principles. We also proposed a methodology for application-independent evaluation of the MW vocabulary quality. The experimental results on two real-world motion processing tasks confirm that we are able to construct MW vocabularies which preserve or even slightly increase application effectiveness and significantly improve processing efficiency.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        },
        {
            "text": "We believe that these achievements open new possibilities for efficient analysis of 3D motion data. In the future, we will study more thoroughly the preparation and preprocessing of the short segments, and develop scalable indexing and search algorithms for the MW data. In particular, we plan to enrich the segmentation process to include several segment sizes, which should help us deal with possible speed variability of semantically related motions. Before the actual quantization, the segments can be replaced by characteristic features extracted, e.g., by state-of-the-art neural networks. To index and search MW sequences, we intend to employ the shingling technique and adapted inverted files.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusions"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Towards improved human action recognition using convolutional neural networks and multimodal fusion of depth and inertial sensor data",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ahmad",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "M"
                    ],
                    "last": "Khan",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "20th International Symposium on Multimedia (ISM)",
            "volume": "",
            "issn": "",
            "pages": "223--230",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Learning to reconstruct people in clothing from a single RGB camera",
            "authors": [
                {
                    "first": "T",
                    "middle": [],
                    "last": "Alldieck",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Magnor",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "L"
                    ],
                    "last": "Bhatnagar",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Theobalt",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pons-Moll",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Deep motifs and motion signatures",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Aristidou",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Cohen-Or",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Hodgins",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Chrysanthou",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Shamir",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "ACM Trans. Graph",
            "volume": "37",
            "issn": "6",
            "pages": "1--187",
            "other_ids": {
                "DOI": [
                    "10.1145/3272127.3275038"
                ]
            }
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Deep representation learning for human motion prediction and classification",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Butepage",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Black",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Kragic",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Kjellstrom",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "6158--6166",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "An information retrieval system for motion capture data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Demuth",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "R\u00f6der",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Eberhardt",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "ECIR 2006",
            "volume": "3936",
            "issn": "",
            "pages": "373--384",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "MDPV: metric distance permutation vocabulary",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Dohnal",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Homola",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zezula",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Inf. Retr. J",
            "volume": "18",
            "issn": "1",
            "pages": "51--72",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Using hand gestures for specifying motion queries in sketch-based video retrieval",
            "authors": [
                {
                    "first": "I",
                    "middle": [
                        "A"
                    ],
                    "last": "Kabary",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Schuldt",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "ECIR 2014",
            "volume": "8416",
            "issn": "",
            "pages": "733--736",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-06028-6_84"
                ]
            }
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Efficient unsupervised temporal segmentation of motion data",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kr\u00fcger",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "V\u00f6gele",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Willig",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Yao",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Klein",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Weber",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Multimed",
            "volume": "19",
            "issn": "4",
            "pages": "797--812",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "RGB-D sensing based human action and interaction analysis: a survey",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Cai",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Ju",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Pattern Recogn",
            "volume": "94",
            "issn": "",
            "pages": "1--12",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Skeleton based human action recognition with global context-aware attention LSTM networks",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Duan",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Hu",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "C"
                    ],
                    "last": "Kot",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Image Process",
            "volume": "27",
            "issn": "4",
            "pages": "1586--1599",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Efficient human motion retrieval via temporal adjacent bag of words and discriminative neighborhood preserving dictionary learning",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "He",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Peng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cheung",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "Y"
                    ],
                    "last": "Tang",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Hum.-Mach. Syst",
            "volume": "47",
            "issn": "6",
            "pages": "763--776",
            "other_ids": {
                "DOI": [
                    "10.1109/THMS.2017.2675959"
                ]
            }
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "Introduction to Information Retrieval",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Manning",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Raghavan",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Sch\u00fctze",
                    "suffix": ""
                }
            ],
            "year": 2008,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "DOI": [
                    "10.1017/CBO9780511809071"
                ]
            }
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Documentation Mocap Database HDM05",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "M\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "R\u00f6der",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Clausen",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Eberhardt",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kr\u00fcger",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Weber",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Scalable recognition with a vocabulary tree",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nist\u00e9r",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Stew\u00e9nius",
                    "suffix": ""
                }
            ],
            "year": 2006,
            "venue": "International Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "2161--2168",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "PPP-codes for large-scale similarity searching",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Novak",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zezula",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hameurlain",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "K\u00fcng",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Decker",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lhotska",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "Transactions on Large-Scale Data-and Knowledge-Centered Systems XXIV",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Object retrieval with large vocabularies and fast spatial matching",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Philbin",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Chum",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Isard",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "International Conference on Computer Vision and Pattern Recognition (CVPR)",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Effective and efficient similarity searching in motion capture data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sedmidubsky",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Elias",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zezula",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Multimed. Tools Appl",
            "volume": "77",
            "issn": "10",
            "pages": "12073--12094",
            "other_ids": {
                "DOI": [
                    "10.1007/s11042-017-4859-7"
                ]
            }
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Searching for variable-speed motions in long sequences of motion capture data",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sedmidubsky",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Elias",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zezula",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "Inf. Syst",
            "volume": "80",
            "issn": "",
            "pages": "148--158",
            "other_ids": {
                "DOI": [
                    "10.1016/j.is.2018.04.002"
                ]
            }
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Probabilistic classification of skeleton sequences",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sedmidubsky",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zezula",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hartmann",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Ma",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Hameurlain",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Pernul",
                    "suffix": ""
                },
                {
                    "first": "",
                    "middle": [],
                    "last": "Wagner",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "DEXA 2018",
            "volume": "11030",
            "issn": "",
            "pages": "50--65",
            "other_ids": {
                "DOI": [
                    "10.1007/978-3-319-98812-2_4"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Augmenting spatio-temporal human motion data for effective 3D action recognition",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sedmidubsky",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Zezula",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "21st IEEE International Symposium on Multimedia (ISM)",
            "volume": "",
            "issn": "",
            "pages": "204--207",
            "other_ids": {
                "DOI": [
                    "10.1109/ISM.2019.00044"
                ]
            }
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Video Google: a text retrieval approach to object matching in videos",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sivic",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Zisserman",
                    "suffix": ""
                }
            ],
            "year": 2003,
            "venue": "9th International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "1470--1477",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Bayesian graph convolution LSTM for skeleton based action recognition",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Zhao",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Ji",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision (ICCV)",
            "volume": "",
            "issn": "",
            "pages": "6882--6892",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Relational network for skeletonbased action recognition",
            "authors": [
                {
                    "first": "W",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Huang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                }
            ],
            "year": 2019,
            "venue": "International Conference on Multimedia and Expo (ICME)",
            "volume": "",
            "issn": "",
            "pages": "826--831",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Deep hashing network for efficient similarity retrieval",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Zhu",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Long",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cao",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "30th Conference on Artificial Intelligence (AAAI)",
            "volume": "",
            "issn": "",
            "pages": "2415--2421",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Representing motions by motion words: both data and queries are transformed into MW sequences and efficiently organized and processed by text-based approaches.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "ered equal: match MW : MW \u00d7 MW \u2192 {0, 1}. The Boolean matching of words {A,D} s 2 \u2192 {C,A,B} s 3 \u2192 {C,D} soft-base match MW : s 1 \u2248 s 2 , s 1 \u2248 s 3, s 2 \u2248 Comparison of the hard, soft, and multi-overlay quantization of segments.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Vocabulary quality in relation to vocabulary method and varying vocabulary size: (a) k-medoids, (b) hk-medoids and (c) random pivot selection.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Implementation.The distance of a segment s located in the Voronoi cell of pivot p 1 to the borderline of the cell of p 2 is estimated as |DT W (p1,s)\u2212DT W (p2,s",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Quality of vocabulary in relation to vocabulary construction method: (a) soft quantization for 350 pivots: varying K for D10 and varying D for K6; (b) multi-overlay quantization: 1 to 5 overlays with 350 pivots each, varying number of matching overlays.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Comparison of F \u03b2 score and actual effectiveness (accuracy, recall) for selected vocabularies in the (a) classification and (b) search applications. (Color figure online)",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Effectiveness of classification and search applications with different segment representations (MW representations use the best-ranked vocabularies with 350 pivots).",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "Acknowledgements. This research has been supported by the GACR project No. GA19-02033S.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "acknowledgement"
        }
    ]
}