{
    "paper_id": "9ca7cf85383ef6628658ba23ee9f023f96ce93b2",
    "metadata": {
        "title": "Relative Lempel-Ziv Compression of Genomes for Large-Scale Storage and Retrieval",
        "authors": [
            {
                "first": "Shanika",
                "middle": [],
                "last": "Kuruppu",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National ICT",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": "kuruppu@csse.unimelb.edu.au"
            },
            {
                "first": "Simon",
                "middle": [
                    "J"
                ],
                "last": "Puglisi",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "Royal Melbourne Institute of Technology",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": "simon.puglisi@rmit.edu.au"
            },
            {
                "first": "Justin",
                "middle": [],
                "last": "Zobel",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "National ICT",
                    "location": {
                        "country": "Australia"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "Self-indexes -data structures that simultaneously provide fast search of and access to compressed text -are promising for genomic data but in their usual form are not able to exploit the high level of replication present in a collection of related genomes. Our 'RLZ' approach is to store a self-index for a base sequence and then compress every other sequence as an LZ77 encoding relative to the base. For a collection of r sequences totaling N bases, with a total of s point mutations from a base sequence of length n, this representation requires just nH k (T ) + s log n + s log N s + O(s) bits. At the cost of negligible extra space, access to consecutive symbols requires O( + log n) time. Our experiments show that, for example, RLZ can represent individual human genomes in around 0.1 bits per base while supporting rapid access and using relatively little memory.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "The emergence of high-throughput sequencing technologies, capable of sequencing entire genomes in a single run, has lead to a dramatic change in the number and type of sequencing projects being undertaken. In particular, it is now feasible to acquire and study variations between many individual genomes of organisms from the same species. While the total size of these sets of genomes will be large, individual genomes will not greatly vary, and so these collections represent new challenges for compression and indexing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "In this paper we address the following problem.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Introduction"
        },
        {
            "text": "Given a collection C of r sequences T k \u2208 C such that |T k | = n for 1 \u2264 k \u2264 r and copies of the base sequence T 1 containing overall s point mutations, the repetitive collection indexing problem is to efficiently store C while allowing queries of the form display(i, j, k) to efficiently return the substring T k [i..j].",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 ([7])."
        },
        {
            "text": "We describe a solution to this problem that requires nH k (T 1 )+s log n+s log N s + O(s) bits of space and O( + log 1+ n) time to return a requested substring of length for any constant > 0, assuming a constant alphabet size.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Definition 1 ([7])."
        },
        {
            "text": "Our approach is to use the base sequence as a dictionary for compression of the other sequences. The collection is parsed into factors in an LZ77 [11] manner, but references to substituted strings are restricted to be in the base sequence only. Arbitrary substrings can then be decoded by reference to the base sequence. This work is inspired by the recent work of M\u00e4kinen et al. [7] who, to our knowledge, were the first to tackle the above problem.",
            "cite_spans": [
                {
                    "start": 146,
                    "end": 150,
                    "text": "[11]",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 380,
                    "end": 383,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [],
            "section": "Definition 1 ([7])."
        },
        {
            "text": "BioCompress [5] was the first compression algorithm to be specific to DNA sequence compression, by simple modifications such as encoding nucleotides with 2 bits per base and detecting reverse complement repeats. Two further variations on the BioCompress theme, Cfact [10] and Off-line [1] , both work in rounds, greedily replacing duplicate text with shorter codes. GenCompress [3] showed that by considering approximate repeats the results could be improved. Since GenCompress, most DNA compression algorithms have been based on efficient methods of approximate repeat detection.",
            "cite_spans": [
                {
                    "start": 12,
                    "end": 15,
                    "text": "[5]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 267,
                    "end": 271,
                    "text": "[10]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 285,
                    "end": 288,
                    "text": "[1]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 378,
                    "end": 381,
                    "text": "[3]",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Background and Basic Tools"
        },
        {
            "text": "Many of these early DNA compression algorithms are only able to compress small files. Due to the latest advances in DNA sequencing technologies, much larger DNA sequencing projects are underway. As a result, many individual genomes from the same species are being sequenced, creating a large amount of redundancy. Recent algorithms have specifically addressed the issue of compressing DNA data from the same species. Christley et al. [4] compresses variation data from human genomes and encodes the mutations and indels with respect to the human reference sequence and known variations recorded in a SNP database. However, large resources are required to be shared among users of the compressed data (4.2 GB of reference and SNPs in Chirstley et al.'s software). Furthermore, it does not support random access into the sequences.",
            "cite_spans": [
                {
                    "start": 434,
                    "end": 437,
                    "text": "[4]",
                    "ref_id": "BIBREF3"
                }
            ],
            "ref_spans": [],
            "section": "Background and Basic Tools"
        },
        {
            "text": "Before explaining our method we introduce notation and review several pieces of algorithmic machinery on which our results rely. For convenience, we assume no factors are generated by rule (a) above; that is, if c occurs in T i for i \u2265 2 then c also occurs in T 1 . If T 1 is not so composed we can simply add the at most \u03c3 \u2212 1 missing symbols to the end of it.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Background and Basic Tools"
        },
        {
            "text": "Compressed Integer Sets. We make use of a compressed set representation due to Okanohara and Sadakane [9] (called \"sdarray\" in their paper). Given a set S of m integers over a universe u this data structure supports the opera- ",
            "cite_spans": [
                {
                    "start": 102,
                    "end": 105,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Background and Basic Tools"
        },
        {
            "text": "We now describe how to store a collection of related sequences, while still allowing efficient access to arbitrary substrings of any of the constituent sequences. Proof. T 1 , the base sequence, is stored in n log \u03c3 bits (in the obvious way). Let Z i = LZ(T i |T 1 ) = (p 1 , 1 ), (p 2 , 2 ), . . . , (p z , zi ) for i > 1 be the parsing of text T i relative to T 1 . We store Z i in two pieces. The position components of each factor, p 1 , . . . , p zi are stored in a table P i [1..z i ] taking z i log n bits. P i is simply a concatenation of the bits representing p 1 , . . . , p z . Each entry in P i is log n bits long, allowing access to any entry p j = P i [j] in O(1) time. The length components are stored in a compressed integer set, L i , containing the values j = u k=1 k for all u \u2208 1..z i . In other words, the values in L i are the starting positions of factors in T i . Via a select query, L i allows us to access a given p j as P i [select(L i , j)]; and the length of the jth factor, j , is simply select(L i , j + 1) \u2212 select(L i , j) for j \u2208 1..z \u2212 1 (because of the terminating sentinel we always have z = 1). Furthermore, the factor that T i [j] falls in is given by rank(L i , j). L i is stored in the \"sdarray\" representation [9] , which requires z log N z + O(z) bits and allows rank in O(log N z ) time and select in O(1). As described, P i and L i allow, given j, fast access to p j and j : simply a select query on L i to get j and a lookup on P i to retrieve p j . Given a position k in sequence T i , we can determine the factor in which k lies by issuing a rank query on L i , in particular rank(L i , k). To find a series of consecutive factors we only need to use rank in obtaining the first factor. For the others, the values can be retrieved using repeated select queries on L i and the p values by accessing consecutive fields in P i , both in constant time per factor. This observation allows us to extract any substring T i [s..e] in O(e \u2212 s + log N z ) time overall. We can reduce the n log \u03c3 term in the size of the above data structure to nH k bits by storing T 1 as a self-index instead of in plain form. This increases the cost to access a substring of length to O( log 1+ n + log N z ) worst-case time. It is possible to build our data structure in O(n + N log n) time and n log \u03c3 + n log n bits of extra space. The basic idea is to build the suffix array for T 1 and \"stream\" every other sequence against it to generate the LZ(T i |T 1 ) parsings.",
            "cite_spans": [
                {
                    "start": 1252,
                    "end": 1255,
                    "text": "[9]",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [],
            "section": "Storing and Accessing Related Genomes"
        },
        {
            "text": "C of r sequences T k \u2208 C such that |T k | = n for 1 \u2264 k \u2264 r and r k=1 |T k | = r \u00b7 n = N , with LZ(T i |T 1 ) = (p 1 , 1 ), (p 2 , 2 ), . . . , (p zi , zi ) , for 2 \u2264 i \u2264 r and z = r i=2 z i ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Lemma 2. Given a collection"
        },
        {
            "text": "The compression performance of our algorithm, which we call RLZ, is compared to the algorithms XM [2] , which is known to currently be the best single sequence DNA compression algorithm, Comrad [6] , which specialises in compression of large related DNA datasets, and RLCSA, an implementation of a self-index from M\u00e4kinen et al. [7] . The RLZ display() function is also compared to RLCSA [7] . Table 1 compares RLZ with RLCSA, Comrad and XM by compressing datasets containing many real biological sequences that slightly vary from each other. The datasets are S. coronavirus with 141 sequences, S. cerevisiae with 39 genomes, S. paradoxus with 36 genomes, and H. sapien with 4 genomes.",
            "cite_spans": [
                {
                    "start": 98,
                    "end": 101,
                    "text": "[2]",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 194,
                    "end": 197,
                    "text": "[6]",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 329,
                    "end": 332,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 388,
                    "end": 391,
                    "text": "[7]",
                    "ref_id": "BIBREF6"
                }
            ],
            "ref_spans": [
                {
                    "start": 394,
                    "end": 401,
                    "text": "Table 1",
                    "ref_id": "TABREF1"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "The compression performance of RLZ is varied (Table 2) . Unsurprisingly, for the smaller datasets, XM has the best compression results. For the larger dataset, RLZ is produces better compression results than Comrad while using less memory (\u224845 Mbyte for the yeast sets). RLCSA doesn't perform as well as the other algorithms, but it uses less memory (\u2248100 Mbyte for yeast) and is faster than Comrad and XM. Overall, RLZ compress and decompress much faster, and uses drastically less memory, than RLCSA, Comrad and XM.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 45,
                    "end": 54,
                    "text": "(Table 2)",
                    "ref_id": null
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "For the H. sapien dataset, each chromosome of each dataset was compressed against the respective reference genome chromosomes for RLZ and RLCSA. We do not report XM results since it took nearly 6 hours for a single chromosome 1 sequence to compress. RLZ performed very well on this dataset compared to RLCSA and Comrad (RLZ used \u22481 Gbyte of memory while RLCSA and Comrad used \u22482 and \u224816 Gbyte respectively). With the inclusion of the 7-zipped human reference genome, the total dataset of three human genome sequences (summing to 12 Gbase) can be represented in just under 755 MByte with RLZ; of this, 643 Mbyte is the overhead for the base sequence, and we anticipate that roughly 27,000 human genomes could be stored in a terabyte, a massive increase on the 1500 or so that could be stored using methods such as 7-zip.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "Results for the display(i,s,e) (retrieve substring T i [s..e]) function are in Table 3 . RLZ displays substrings significantly faster than RLCSA for small query lengths and continues its good performance for even larger query lengths, with approximately 0.022 (\u03bcsec/c) for RLZ compared to 0.77 (\u03bcsec/c) for RLCSA.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 79,
                    "end": 86,
                    "text": "Table 3",
                    "ref_id": "TABREF2"
                }
            ],
            "section": "Experimental Results"
        },
        {
            "text": "Tests were conducted on a 2.6 GHz Dual-Core AMD Opteron CPU with 32Gb RAM and 512K cache running Ubuntu 8.04 OS. The compiler was gcc v4.2.4 with the -O9 option.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Experimental Results"
        },
        {
            "text": "A key issue is choice of a reference sequence. While selecting the reference genome is a simple matter for the yeast datasets, it may not be a good representation of the other individual sequences. To observe the compression performance for different reference sequence choices, we compressed the dataset by selecting each genome in turn as the reference. Selecting a genome such as DBVPG6765 leads to 16.5 MByte as opposed to selecting UWOPS05 227 2, which led to 24.5 MByte. We also explored the effect on compression when a reference sequence that is unrelated to the original dataset is selected. 35 genomes of S. paradoxus (excluding REF genome) was compressed against the S. cerevisiae REF genome. The compressed size was 112.09 MByte compared to the result of 2.04 MByte when the S. paradoxus REF genome was used. RLZ's effectiveness is at its best when compressing related sequences and care must be taken when selecting a reference sequence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Compression of biological sequences by greedy off-line textual substitution",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Apostolico",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Lonardi",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Proc. IEEE DCC",
            "volume": "",
            "issn": "",
            "pages": "143--152",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "A simple statistical algorithm for biological sequence compression",
            "authors": [
                {
                    "first": "M",
                    "middle": [
                        "D"
                    ],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Dix",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Allison",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Mears",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proc. IEEE DCC",
            "volume": "",
            "issn": "",
            "pages": "43--52",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "A compression algorithm for DNA sequences and its applications in genome comparison",
            "authors": [
                {
                    "first": "X",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kwong",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                }
            ],
            "year": 2000,
            "venue": "Proc. RECOMB",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF3": {
            "ref_id": "b3",
            "title": "Human genomes as email attachments",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Christley",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "X",
                    "middle": [],
                    "last": "Xie",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Bioinformatics",
            "volume": "25",
            "issn": "2",
            "pages": "274--275",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Compression of DNA sequences",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Grumbach",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Tahi",
                    "suffix": ""
                }
            ],
            "year": 1993,
            "venue": "Proc. IEEE DCC",
            "volume": "",
            "issn": "",
            "pages": "340--350",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Repetition-based compression of large DNA datasets",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Kuruppu",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Beresford-Smith",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Conway",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Zobel",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Poster at RECOMB",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Storage and retrieval of highly repetitive sequence collections",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "M\u00e4kinen",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Navarro",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Sir\u00e9n",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "V\u00e4lim\u00e4ki",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "J. Computational Biology",
            "volume": "17",
            "issn": "3",
            "pages": "281--308",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "Compressed full text indexes",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Navarro",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "M\u00e4kinen",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "ACM Computing Surveys",
            "volume": "39",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Practical entropy-compressed rank/select dictionary",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Okanohara",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Sadakane",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "Proc. ALENEX. SIAM",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "A guaranteed compression scheme for repetitive DNA sequences",
            "authors": [
                {
                    "first": "E",
                    "middle": [],
                    "last": "Rivals",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Delahaye",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dauchet",
                    "suffix": ""
                },
                {
                    "first": "O",
                    "middle": [],
                    "last": "Delgrange",
                    "suffix": ""
                }
            ],
            "year": 1996,
            "venue": "Proc. IEEE DCC",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "A universal algorithm for sequential data compression",
            "authors": [
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ziv",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Lempel",
                    "suffix": ""
                }
            ],
            "year": 1977,
            "venue": "IEEE Transactions on Information Theory",
            "volume": "23",
            "issn": "3",
            "pages": "337--343",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "|T k | = N , where T 2 , T 3 , . . . , T r are mutated This work was supported by the Australian Research Council and the NICTA Victorian Research Laboratory. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Center of Excellence program.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Strings. A string T = T [0..n] = T [0]T [1] . . . T [n] is a sequence of n + 1 = |T | symbols. The first n symbols of T are drawn from a constant ordered alphabet, \u03a3. The final character T [n] is a special \"end of string\" character, $, distinct from and lexicographically smaller than all the other characters in \u03a3. We write T [i..j] to represent the substring T [i]T [i +1] \u00b7 \u00b7 \u00b7 T [j] of T that starts at position i and ends at position j. For substring T [i..j], if j = n (resp. i = 0) we call the substring a suffix (resp. prefix) of T . WithT we denote the reverse of T . Suffix Arrays and Self-Indexes. The suffix array of T , denoted SA T or just SA, when the context is clear, is an array SA[0..n] that contains a permutation of the integers 0..n such that T [SA[0]..n] < T [SA[1]..n] < \u00b7 \u00b7 \u00b7 < T [SA[n]..n]. In other words, SA[j] = i iff T [i..n] is the jth suffix of T in ascending lexicographical order. All the positions of occurrence in T of a given pattern, P [1..m], lie in a contiguous range of the suffix array SA[sp..ep]. In recent years, successful attempts have been made to compress suffix arrays, and data structures call self-indexes have emerged [8]. A self-index of a text T allows the following functionality, all in compressed space: (i) extract (display) any substring T [s..e], (ii) find the range sp..ep for a pattern P , and (iii) return SA[i] for any i. Relative Lempel-Ziv Factorization. Given two strings T and S, the Lempel-Ziv factorization (or parsing) of T relative to S, denoted LZ(T |S), is a factorization T = w 0 w 1 w 2 . . . w z where w 0 is the empty string and for i > 0 each factor (string) w i is either: (a) a letter which does not occur in S; or otherwise (b) the longest prefix of T [|w 0 . . . w i\u22121 |..|T |] that occurs as a substring of S. For example, if S = abaababa and T = aabacaab then in LZ(T |S) we have w 1 = aaba, w 2 = c and w 3 = aab. It is convenient to specify the factors not as strings, but as (p i , i ) pairs, where p i denotes the starting position in S of an occurrence 1 of factor w i (or a letter if w i is by rule (a)) and i denotes the length of the factor (or is zero if w i is by rule (a)). Thus, in our example: LZ(T |S) = (3, 4)(c, 0)(2, 3).",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "tions: rank(S, i), returning the number of item in S less than or equal to i; and select(S, i), returning the value of the ith item in S. The data structure requires m log u m + O(m) bits and O(1) time for select and O(log u m ) time for rank.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "we can store C in at most n log \u03c3 + z log n +",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Compression results for four repetitive collections. The first row is the original size for all datasets (Size in Megabases rather than Mbytes), the remaining rows are the compression performance of RLZ, RLCSA, Comrad and XM algorithms. The two columns per dataset show the size in Mbytes and the 0-order entropy (in bits per base).",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": []
}