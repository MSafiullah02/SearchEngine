{
    "paper_id": "PMC7206236",
    "metadata": {
        "title": "Mask-Guided Region Attention Network for Person Re-Identification",
        "authors": [
            {
                "first": "Hady",
                "middle": [
                    "W."
                ],
                "last": "Lauw",
                "suffix": "",
                "email": "hadywlauw@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Raymond",
                "middle": [
                    "Chi-Wing"
                ],
                "last": "Wong",
                "suffix": "",
                "email": "raywong@cse.ust.hk",
                "affiliation": {}
            },
            {
                "first": "Alexandros",
                "middle": [],
                "last": "Ntoulas",
                "suffix": "",
                "email": "antoulas@di.uoa.gr",
                "affiliation": {}
            },
            {
                "first": "Ee-Peng",
                "middle": [],
                "last": "Lim",
                "suffix": "",
                "email": "eplim@smu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "See-Kiong",
                "middle": [],
                "last": "Ng",
                "suffix": "",
                "email": "seekiong@nus.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Sinno",
                "middle": [
                    "Jialin"
                ],
                "last": "Pan",
                "suffix": "",
                "email": "sinnopan@ntu.edu.sg",
                "affiliation": {}
            },
            {
                "first": "Cong",
                "middle": [],
                "last": "Zhou",
                "suffix": "",
                "email": "519658713@qq.com",
                "affiliation": {}
            },
            {
                "first": "Han",
                "middle": [],
                "last": "Yu",
                "suffix": "",
                "email": "han.yu@njupt.edu.cn",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Person re-identification (ReID) aims to identify the same individual across multiple cameras. In general, it is considered as a sub-problem of image retrieval. Given a query image containing a target pedestrian, ReID is to rank the gallery images and search for the same pedestrian. It plays an important role in various surveillance applications, such as intelligent security and pedestrian tracking.",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In the past years, many methods [1\u20134] have been proposed to address the ReID problem. However, it still remains as an incomplete task due to large pose variations, complex background clutters, various camera views, severe occlusions and uncontrollable illumination conditions. Recently, with the improvement of human pose estimation [5\u20137], some researches [8\u201310] utilize the estimation results as spatial attention maps to learn features from pedestrian body parts and then align them. These methods achieve great success and prove that extracting features exactly from body regions rather than background regions is helpful for ReID.",
            "cite_spans": [
                {
                    "start": 33,
                    "end": 34,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 35,
                    "end": 36,
                    "mention": "4",
                    "ref_id": "BIBREF32"
                },
                {
                    "start": 334,
                    "end": 335,
                    "mention": "5",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 336,
                    "end": 337,
                    "mention": "7",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 357,
                    "end": 358,
                    "mention": "8",
                    "ref_id": "BIBREF36"
                },
                {
                    "start": 359,
                    "end": 361,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "However, there are still notable problems in these methods concluded as follows. 1) As shown in Fig. 1, these methods tend to extract features from imprecise part shapes set by handcraft, such as patches [1, 11] and rectangular regions of interest (RoIs) [9, 12], which can introduce noise. 2) Part-level feature alignment which means matching two pedestrians with their heads, arms, legs, and other body parts is improper for ReID. 3) Feature representation is not accurate and comprehensive enough.\n",
            "cite_spans": [
                {
                    "start": 205,
                    "end": 206,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 208,
                    "end": 210,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 256,
                    "end": 257,
                    "mention": "9",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 259,
                    "end": 261,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 101,
                    "end": 102,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "In the first problem, the main reason that the handcrafted shapes cannot precisely describe the silhouettes of body parts is that the shapes of body parts are irregular. Feature alignment based on these shapes can introduce noise from background clutters, occlusions and even adjacent parts as in Fig. 1, leading to inaccurate matching. To deal with this problem, we propose to use pedestrian masks as the spatial attention maps for masking out clutters and meanwhile obtaining the finer silhouettes of body parts both in pixel-level, as shown in Fig. 2(a). These silhouettes obtained by pedestrian masks should be more precise and closer to the reality of body shapes. For the second problem, the works mentioned above generally align features based on part-level and this is inappropriate for ReID. As walking is a dynamic process, and in this process, the moving arms and legs have huge morphological changes and often cause heavy self-occlusion, which implies that a body part will inevitably be occluded by other parts. For example, left legs are often occluded by right legs. Due to self-occlusion, it is difficult to align features based on part-level. Furthermore, each pedestrian has his own walking postures that are different from others\u2019, which means his head, upper body and lower body have their own morphological characters when walking. But the part-level alignment may discard these characters, as shown in Fig. 2(b). Meanwhile, the head, upper body and lower body are generally separate from each other in a walking pedestrian, which indicates there is no self-occlusion among these three parts as demonstrated in Fig. 2(b).\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 302,
                    "end": 303,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 552,
                    "end": 553,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1429,
                    "end": 1430,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 1637,
                    "end": 1638,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Based on the above analysis, it is concluded that region-level feature alignment based on head, upper body and lower body is more reasonable for ReID. Furthermore, apart from self-occlusion, pedestrians may have some carry-on items, such as backpacks, handbags and caps. These items are definitely helpful for ReID and we can treat them as special parts of pedestrians, which should be included in the corresponding local region like in Fig. 2(b). In the third problem, these methods like [1, 9, 12] only align the part features, considered as local features, and the global feature of the whole pedestrian region is not considered. However, each pedestrian is intuitively associated with a global feature including body shape, walking posture and so on, which cannot be replaced by local features. Due to the neglect of global features, the final feature representation will not be comprehensive and robust enough. Meanwhile, previous works [13, 14] extract the global feature from the entire pedestrian image including background clutters and occlusions, which will introduce noise and lead to the inaccuracy of feature representation. Here, we utilize pedestrian masks to redesign the global features, removing clutters with masks firstly and then extracting the global features of pedestrians. After these operations, multi-feature fusion can be used to align features.",
            "cite_spans": [
                {
                    "start": 490,
                    "end": 491,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 493,
                    "end": 494,
                    "mention": "9",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 496,
                    "end": 498,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 943,
                    "end": 945,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 947,
                    "end": 949,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Introduction",
            "ref_spans": [
                {
                    "start": 442,
                    "end": 443,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Based on above motivations, we propose a new Mask-Guided Region Attention Network for person re-identification. The contributions of our work can be summarized as follows:To make the better use of feature alignment technique for person re-identification, a unified framework called Mask-Guided Region Attention Network (MGRAN) is proposed.To further reduce the noise from background clutters and occlusions, we explore to utilize masks to separate pedestrians from them and obtain the finer silhouettes of pedestrian bodies.Region-level feature alignment, based on head, upper body and lower body, is introduced as a more appropriate method for ReID.We redesign the global feature and utilize multi-feature fusion to improve the accuracy and the completeness of feature representation.\n",
            "cite_spans": [],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Recently, person re-identification methods based on deep learning achieved great success [13, 15, 16]. In general, these methods can be classified into two categories, namely feature representation and distance metric learning. The first category [1, 3, 17, 18] often treats ReID as a classification problem. These methods dedicate to design view-invariant representations for pedestrians. The second category [19\u201321] mainly aims at measuring the similarity between pedestrian images by learning a robust distance metric.",
            "cite_spans": [
                {
                    "start": 90,
                    "end": 92,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 94,
                    "end": 96,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 98,
                    "end": 100,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 248,
                    "end": 249,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 251,
                    "end": 252,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 254,
                    "end": 256,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 258,
                    "end": 260,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 411,
                    "end": 413,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 414,
                    "end": 416,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Person Re-Identification ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Among these methods, many of them [9, 12] achieved the success by feature alignment. Numerous studies proved the importance of feature alignment for ReID. For example, Su et al. [5] proposed a Pose-driven Deep Convolutional model (PDC) that used Spatial Transformer Network (STN) to crop body regions based on pre-defined centers. Xu et al. [9] achieved the more precise feature alignment based on their proposed network called Attention-Aware Compositional Network (AACN) and further improved the performance of identification. However, these methods align the part features based on the body shapes set by handcraft, which is usually imprecise. In our model, we utilize pedestrian masks in pixel-level to align features, intending to obtain more precise information of body parts.",
            "cite_spans": [
                {
                    "start": 35,
                    "end": 36,
                    "mention": "9",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 38,
                    "end": 40,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 179,
                    "end": 180,
                    "mention": "5",
                    "ref_id": "BIBREF33"
                },
                {
                    "start": 342,
                    "end": 343,
                    "mention": "9",
                    "ref_id": "BIBREF37"
                }
            ],
            "section": "Person Re-Identification ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "With the rapid development of instance segmentation based on deep learning methods such as Mask R-CNN [22] and the Fully Convolutional Networks (FCN) [23], now we can easily obtain high-quality pedestrian masks which can be used in person re-identification. Furthermore, these instance segmentation methods can be naturally extended to human pose estimation by modeling keypoint locations as one-hot masks. We can further improve the performance of person re-identification by integrating the results of instance segmentation and human pose estimation.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 105,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 151,
                    "end": 153,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                }
            ],
            "section": "Instance Segmentation and Human Pose Estimation ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "Spatial attention mechanism has achieved great success in understanding images and it has been widely used in various tasks, such as semantic segmentation [24], object detection [25] and person re-identification [26]. For example, Chu et al. [6] proposed a multi-context attention model for pose estimation. Inspired by these methods, we use spatial attention maps to remove the undesirable clutters in pedestrian images. However, different from them, we use binary pedestrian masks as spatial attention maps to obtain more precise information of pedestrian bodies.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 158,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 179,
                    "end": 181,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 213,
                    "end": 215,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 243,
                    "end": 244,
                    "mention": "6",
                    "ref_id": "BIBREF34"
                }
            ],
            "section": "Spatial Attention Mechanism ::: Related Work",
            "ref_spans": []
        },
        {
            "text": "The overall framework of our Mask-Guided Region Attention Network (MGRAN) is illustrated in Fig. 3. MGRAN consists of two main components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA).\n",
            "cite_spans": [],
            "section": "Overall Architecture ::: Mask-Guided Region Attention Network",
            "ref_spans": [
                {
                    "start": 97,
                    "end": 98,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "The MRA module aims to generate two kinds of attention maps: pedestrian masks and human body keypoints. It is constructed by a two-branch neural network, which predicts the attention maps of the pedestrians and their keypoints, respectively.",
            "cite_spans": [],
            "section": "Overall Architecture ::: Mask-Guided Region Attention Network",
            "ref_spans": []
        },
        {
            "text": "The MA module is constructed by a four-branch neural network. It utilizes the estimated attention maps to extract global features and local features. A series of extracted features are then fused for multi-feature alignment.",
            "cite_spans": [],
            "section": "Overall Architecture ::: Mask-Guided Region Attention Network",
            "ref_spans": []
        },
        {
            "text": "\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{P} $$\\end{document}. A pedestrian mask \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{P} $$\\end{document} is the encoding of an input image\u2019s spatial layout. It is a binary encoding which means that the pixels of pedestrian region are encoded as number 1 and the others are encoded as number 0. Following the original article of Mask R-CNN, we set hyper-parameters as suggested by existing Faster R-CNN work [27] and define the loss \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{L}_{{\\varvec{mask}}} \\left( \\varvec{P} \\right) $$\\end{document} on each sampled RoI in Mask R-CNN as the average binary cross-entropy loss,1\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ L_{mask} \\left( \\varvec{P} \\right)\\, = \\, - \\frac{1}{N}\\sum\\nolimits_{i = 1}^{N} {\\varvec{P}_{\\varvec{i}}^{\\varvec{*}} \\, \\cdot \\,\\log \\left( {\\sigma \\left( {\\varvec{P}_{\\varvec{i}} } \\right)} \\right)\\, + \\,\\left( {1\\, - \\,\\varvec{P}_{\\varvec{i}}^{\\varvec{*}} } \\right)\\, \\cdot \\,\\log \\left( {1\\, - \\,\\sigma \\left( {\\varvec{P}_{\\varvec{i}} } \\right)} \\right),} $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ N $$\\end{document} is the number of pixels in a predicted mask, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\sigma $$\\end{document} denotes the sigmoid function, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{P}_{\\varvec{i}} $$\\end{document} is a single pixel in the mask, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{P}_{\\varvec{i}}^{\\varvec{*}} $$\\end{document} is the corresponding ground truth pixel. Furthermore, the classification loss \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ L_{cls} $$\\end{document} and the bounding-box loss \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ L_{box} $$\\end{document} of each sampled RoI are set as indicated in [21].",
            "cite_spans": [
                {
                    "start": 918,
                    "end": 920,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 4080,
                    "end": 4082,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                }
            ],
            "section": "Pedestrian Masks ::: Mask-Guided Region Attention ::: Mask-Guided Region Attention Network",
            "ref_spans": []
        },
        {
            "text": "Mask R-CNN can easily be extended to keypoints detection. We model a keypoint\u2019s location as a one-hot mask and use Mask R-CNN to predict four masks, one for each of the four keypoints as shown in Fig. 4. Following the original article of Mask R-CNN, during training, we minimize the cross-entropy loss over an \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{m}^{2} $$\\end{document}-way softmax output for each visible ground-truth keypoint, which encourages a single point to be detected.",
            "cite_spans": [],
            "section": "Keypoint Masks K. ::: Mask-Guided Region Attention ::: Mask-Guided Region Attention Network",
            "ref_spans": [
                {
                    "start": 201,
                    "end": 202,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "Space Alignment aims to obtain the pedestrian region and the three local regions. Based on the attention masks generated by MRA module, we propose a simple and effective approach to obtain them. Specifically, we firstly apply Hadamard Product between the original image \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{M} $$\\end{document} and the corresponding pedestrian mask \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{P} $$\\end{document} to obtain the pedestrian region, as follows:2\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{M}^{\\varvec{*}} = \\varvec{M} \\circ \\varvec{P} , $$\\end{document}where, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\circ $$\\end{document} denotes the Hadamard Product operator which performs element-wise product on two matrices or tensors and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{M}^{\\varvec{*}} $$\\end{document} denotes the pedestrian region. It is worth noting that we use Hadamard Product on the original image to guarantee the accuracy of features. Some works [9, 12] use spatial attention maps on processed data such as data processed by convolution, which will introduce noise into the attention region from other regions in the image. Secondly, based on the obtained pedestrian body region, we utilize the four keypoint masks to obtain the three local regions by connecting two adjacent keypoints and segmenting the pedestrian region, as shown in Fig. 4.",
            "cite_spans": [
                {
                    "start": 2159,
                    "end": 2160,
                    "mention": "9",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 2162,
                    "end": 2164,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Space Alignment (SA). ::: Multi-feature Alignment ::: Mask-Guided Region Attention Network",
            "ref_spans": [
                {
                    "start": 2553,
                    "end": 2554,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "In this module, we use four ResNet-50 networks [28] to extract the features of the four regions generated by SA module, respectively. Then feature fusion is used to align features, as follows:3\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{F}\\, = \\,Concat\\left( {\\left\\{ {\\varvec{f}_{\\varvec{g}} \\,\\; \\varvec{f}_{\\varvec{l}}^{1} \\; \\varvec{f}_{\\varvec{l}}^{2} \\,\\,\\varvec{f}_{\\varvec{l}}^{3} } \\right\\}} \\right) , $$\\end{document}where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Concat\\left( \\cdot \\right) $$\\end{document} denotes the concatenation operation on feature vectors, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{f}_{\\varvec{g}} $$\\end{document} represents the global feature of the whole pedestrian body region, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{f}_{\\varvec{l}}^{1} $$\\end{document}, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{f}_{\\varvec{l}}^{2} $$\\end{document} and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{f}_{\\varvec{l}}^{3} $$\\end{document} denote the features of the three local regions respectively, and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\varvec{F} $$\\end{document} is the final feature vector for the input pedestrian image.",
            "cite_spans": [
                {
                    "start": 48,
                    "end": 50,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                }
            ],
            "section": "Multi-feature Fusion (MF). ::: Multi-feature Alignment ::: Mask-Guided Region Attention Network",
            "ref_spans": []
        },
        {
            "text": "Overall, our framework integrated the MRA and MA to extract features for input pedestrian images.",
            "cite_spans": [],
            "section": "Multi-feature Fusion (MF). ::: Multi-feature Alignment ::: Mask-Guided Region Attention Network",
            "ref_spans": []
        },
        {
            "text": "We construct the Mask R-CNN model with a ResNet-50-FPN backbone and use the annotated person images in the COCO dataset [29] to train it. Furthermore, the floating-number mask output is binarized at a threshold of 0.5. In MF, the four ResNet-50 networks share the same parameters and we use the Margin Sample Mining Loss (MSML) [30] to conduct distance metric learning based on the four features extracted by ResNet-50. We scale the all images input into Mask R-CNN and ResNet-50 with a factor of 1/256. Finally, MRA and MA are trained independently.",
            "cite_spans": [
                {
                    "start": 121,
                    "end": 123,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 329,
                    "end": 331,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Implementation Details ::: Mask-Guided Region Attention Network",
            "ref_spans": []
        },
        {
            "text": "We evaluate our method on three large-scale public person ReID datasets, including Market-1501 [31], DukeMTMC-reID [32] and CUHK03 [1], details of them are shown in Table 1. For fair comparison, we follow the official evaluation protocols of each dataset. For Market-1501 and DukeMTMC-reID, rank-1 identification rate (%) and mean Average Precision (mAP) (%) are used. For CUHK03, Cumulated Matching Characteristics (CMC) at rank-1 (%) and rank-5 (%) are adopted.\n",
            "cite_spans": [
                {
                    "start": 96,
                    "end": 98,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 116,
                    "end": 118,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 132,
                    "end": 133,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "section": "Datasets and Protocols ::: Experiments",
            "ref_spans": [
                {
                    "start": 171,
                    "end": 172,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "We choose 13 methods in total with state-of-the-art performance for comparisons with our proposed framework MGRAN. These methods can be categorized into two classes according to whether human pose information is used. The Spindle-Net (Spindle) [12], Deeply-Learned Part-Aligned Representations (DLPAR) [10], MSCAN [33], and the Attention-Aware Compositional Network (AACN) [9] are pose-relevant. The Online Instance Matching (OIM) [14], Re-ranking [34], the deep transfer learning method (Transfer) [35], the SVDNet [15], the pedestrian alignment network (PAN) [36], the Part-Aligned Representation (PAR) [10], the Deep Pyramid Feature Learning (DPFL) [13], DaF [37] and the null space semi-supervised learning method (NFST) [38] are pose-irrelevant. The experimental results are presented in Table 2, 3 and 4.\n\n\n",
            "cite_spans": [
                {
                    "start": 245,
                    "end": 247,
                    "mention": "12",
                    "ref_id": "BIBREF3"
                },
                {
                    "start": 303,
                    "end": 305,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 315,
                    "end": 317,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 374,
                    "end": 375,
                    "mention": "9",
                    "ref_id": "BIBREF37"
                },
                {
                    "start": 432,
                    "end": 434,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 449,
                    "end": 451,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 500,
                    "end": 502,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 517,
                    "end": 519,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 562,
                    "end": 564,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 606,
                    "end": 608,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 653,
                    "end": 655,
                    "mention": "13",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 663,
                    "end": 665,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 726,
                    "end": 728,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Comparison with the State-of-the-Art Methods ::: Experiments",
            "ref_spans": [
                {
                    "start": 799,
                    "end": 800,
                    "mention": "2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 802,
                    "end": 803,
                    "mention": "3",
                    "ref_id": "TABREF2"
                },
                {
                    "start": 808,
                    "end": 809,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "Based on the experimental results, it is obvious that our MGRAN framework outperforms the compared methods, showing the advantages of our approach. To be specific, compared with the second best method on each dataset, our framework achieves 6.10%, 1.89%, 1.28%, 7.62% and 6.57% rank-1 accuracy improvement on Market-1501 (Single Query), Market-1501 (Multiple Query), DukeMTMC-reID, CUHK03 (Labeled) and CUHK03 (Detected), respectively. Furthermore, compared with the second best method on Market-1501 and DukeMTMC-reID, 14.40%, 9.58% and 4.32% mAP improvement on Market-1501 (Single Query), Market-1501 (Multiple Query) and DukeMTMC-reID are achieved, respectively.",
            "cite_spans": [],
            "section": "Comparison with the State-of-the-Art Methods ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "We verify the effectiveness of MF on Market-1501 and",
            "cite_spans": [],
            "section": "Multi-feature Fusion (MF). ::: Ablation Analysis ::: Experiments",
            "ref_spans": []
        },
        {
            "text": "DukeMTMC-reID dataset by removing global features in final feature vectors. As shown in Table 5, MF increases the rank-1 accuracy by 2.61%, 2.25% and 0.81% on Market-1501 (Single Query), Market-1501 (Multiple Query) and DukeMTMC-reID. Furthermore, 3.77%, 0.77% and 3.47% mAP improvement on Market-1501 (Single Query), Market-1501 (Multiple Query) and DukeMTMC-reID are achieved based on MF.\n",
            "cite_spans": [],
            "section": "Multi-feature Fusion (MF). ::: Ablation Analysis ::: Experiments",
            "ref_spans": [
                {
                    "start": 94,
                    "end": 95,
                    "mention": "5",
                    "ref_id": "TABREF4"
                }
            ]
        },
        {
            "text": "We align features based on part-level and region-level respectively to verify the effectiveness of our proposed region-level feature alignment. Specifically, we replace region-level feature alignment in MGRAN with part-level feature alignment and keep the other parts unchanged. As shown in Table 6, RFA increases the rank-1 accuracy by 1.19% and 1.32% on CUHK03 (Labeled) and CUHK03 (Detected). Meanwhile, RFA increases the rank-5 accuracy by 1.53% and 1.46% on CUHK03 (Labeled) and CUHK03 (Detected). The experimental results show the usefulness of our proposed RFA.\n",
            "cite_spans": [],
            "section": "Region-Level Feature Alignment (RFA). ::: Ablation Analysis ::: Experiments",
            "ref_spans": [
                {
                    "start": 297,
                    "end": 298,
                    "mention": "6",
                    "ref_id": "TABREF5"
                }
            ]
        },
        {
            "text": "In this paper, we propose a novel Mask-Guided Region Attention Network (MGRAN) for person re-identification to deal with the clutter and misalignment problem. MGRAN consists of two main components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). MRA generates spatial attention maps to mask out undesirable clutters and obtain finer silhouettes of pedestrian bodies. MA aims to align features based on region-level which is more appropriate for ReID. Our method has achieved some success, but with the rapid development of science, a great number of excellent technologies have been created, such as GAN, and in the future work, we propose to use these technologies to further improve the performance of ReID.",
            "cite_spans": [],
            "section": "Conclusion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table\u00a01.: The details of three public datasets used in experiments.\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table\u00a02.: Comparison results on Market-1501 dataset.\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table\u00a03.: Comparison results on DukeMTMC-reID dataset.\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table\u00a04.: Comparison results on CUHK03 dataset.\n",
            "type": "table"
        },
        "TABREF4": {
            "text": "Table\u00a05.: Effectiveness of MF. MGRAN \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ {-} $$\\end{document} GF means removing global features in final feature vectors.\n",
            "type": "table"
        },
        "TABREF5": {
            "text": "Table\u00a06.: Effectiveness of RFA. MGRAN-PL means aligning features based on part-level. MGRAN-RL means aligning features based on region-level.\n",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Fig.\u00a01.: Imprecise shapes of body parts set by handcraft, such as patches [1, 11] and rectangular RoIs [9, 12], include extensive noise.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Fig.\u00a02.: (a): Pedestrian masks can be used to mask out clutters and obtain the finer silhouettes of pedestrian body parts. (b): Pedestrians\u2019 heads, upper bodies and lower bodies have their own morphological characters which can not be presented by a single body part. For example, the morphological characters of upper bodies are presented by arms and upper torsos, such as the amplitude of arm swing (Yellow Arrow). (Color figure online)",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Fig.\u00a03.: Mask-Guided Region Attention Network (MGRAN). Our proposed MGRAN consists of two main components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). MRA aims to generate two types of attention maps: pedestrian masks and human body keypoint masks. MA utilizes the attention maps generated by MRA to obtain the pedestrian region and the three associated local regions. Then the global feature and local features are extracted and multi-feature fusion is used to align them.",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Fig.\u00a04.: Two types of masks: pedestrian masks and keypoint masks. In this paper, we define four keypoints (Blue Dots). By connecting two adjacent keypoints, we can divide the pedestrian region into three local regions: the head, the upper body and the lower body. (Color figure online)",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "Embedding deep metric for person re-identification: a study against large variations",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Shi",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "732-748",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Deep ranking for person re-identification via joint representation learning",
            "authors": [
                {
                    "first": "SZ",
                    "middle": [],
                    "last": "Chen",
                    "suffix": ""
                },
                {
                    "first": "CC",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "JH",
                    "middle": [],
                    "last": "Lai",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "IEEE Trans. Image Process.",
            "volume": "25",
            "issn": "5",
            "pages": "2353-2367",
            "other_ids": {
                "DOI": [
                    "10.1109/TIP.2016.2545929"
                ]
            }
        },
        "BIBREF12": {
            "title": "Deep feature learning with relative distance com-parison for person re-identification",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ding",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Chao",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Pattern Recognit.",
            "volume": "48",
            "issn": "10",
            "pages": "2993-3003",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2015.04.005"
                ]
            }
        },
        "BIBREF13": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "End-to-end comparative attention networks for person re-identification",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Liu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Feng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Qi",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Jiang",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Yan",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "IEEE Trans. Image Process.",
            "volume": "26",
            "issn": "7",
            "pages": "3492-3506",
            "other_ids": {
                "DOI": [
                    "10.1109/TIP.2017.2700762"
                ]
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Microsoft COCO: Common Objects in Context",
            "authors": [
                {
                    "first": "T-Y",
                    "middle": [],
                    "last": "Lin",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "Computer Vision \u2013 ECCV 2014",
            "volume": "",
            "issn": "",
            "pages": "740-755",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF25": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF26": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF28": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF29": {
            "title": "Pedestrian alignment network for large-scale person re-identification",
            "authors": [
                {
                    "first": "Z",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Zheng",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "IEEE Trans. Circ. Syst. Video Technol.",
            "volume": "29",
            "issn": "10",
            "pages": "3037-3045",
            "other_ids": {
                "DOI": [
                    "10.1109/TCSVT.2018.2873599"
                ]
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF32": {
            "title": "Deep attributes driven multi-camera person re-identification",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Su",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Xing",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [],
                    "last": "Gao",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Tian",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "475-491",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF33": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF34": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "Stacked hourglass networks for human pose estimation",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Newell",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Deng",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "Computer Vision \u2013 ECCV 2016",
            "volume": "",
            "issn": "",
            "pages": "483-499",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF36": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}